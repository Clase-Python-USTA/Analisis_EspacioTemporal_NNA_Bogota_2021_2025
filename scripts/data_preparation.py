# ============================================================
# DATA PREPARATION - Proyecto NNA Bogot√° (2021‚Äì2025)
# Limpieza, transformaci√≥n y preparaci√≥n de datos
# ============================================================

import os # Manipulaci√≥n de rutas
import pandas as pd # Manipulaci√≥n de datos
import numpy as np # C√°lculos num√©ricos
import re # Expresiones regulares
import matplotlib.pyplot as plt # Visualizaci√≥n
import seaborn as sns # Visualizaci√≥n avanzada
from datetime import datetime # Manejo de fechas
from dotenv import load_dotenv # Carga variables entorno
import warnings     # Manejo de warnings
import json # Manejo de JSON

# Configuraci√≥n del entorno
warnings.filterwarnings('ignore')
sns.set_style("whitegrid")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("="*70)
print(" DATA PREPARATION - PROYECTO NNA BOGOT√Å (2021-2025)")
print("="*70)
print()

"""
============================================================
RESUMEN EJECUTIVO - DATA PREPARATION
Proyecto: NNA Bogot√° (2021-2025)
============================================================

 PROP√ìSITO:
    Limpia, transforma y prepara los datos para an√°lisis posterior.
    Convierte datos brutos en datos listos para modelado.

 OBJETIVO PRINCIPAL:
    Preparar una base de datos limpia, consistente y sin informaci√≥n
    personal, lista para an√°lisis espacio-temporal.

============================================================
 ESTRUCTURA DEL PROCESO (7 PASOS)
============================================================

PASO 1: CARGA DE DATOS
--------
- Detecta formato del archivo (Excel/CSV)
- Carga hoja 'BD' si existe, si no, la √∫ltima hoja
- Reporta dimensiones y memoria utilizada
- Validaci√≥n inicial del archivo

PASO 2: ESTANDARIZACI√ìN DE COLUMNAS
--------
- Limpia nombres de columnas:
  - Elimina espacios y caracteres especiales
  - Convierte todo a MAY√öSCULAS
  - Reemplaza espacios por guiones bajos (_)
  - Ejemplo: "Nombre Columna!" ‚Üí "NOMBRE_COLUMNA"
- Guarda mapeo de cambios en JSON para trazabilidad
- Elimina guiones bajos duplicados

PASO 3: ELIMINACI√ìN DE INFORMACI√ìN PERSONAL (PII)
--------
- Identifica y elimina columnas con datos personales:
  - Nombres, apellidos, documentos
  - Tel√©fonos, correos, direcciones
  - Informaci√≥n de acudientes
  - Nombres de EPS/EAPB
- EXCEPCIONES: Mantiene columnas como:
  - NUMERO_DE_MANZANA_DEL_CUIDADO
  - NUMERO_DE_FICHA_ANTERIOR
- Reporta columnas eliminadas

PASO 4: LIMPIEZA DE INCONSISTENCIAS
--------
- Limpia texto en columnas categ√≥ricas:
  - Elimina espacios extra
  - Convierte 'nan', 'None', '' a NaN real
- Estandariza fechas a formato datetime
- Elimina registros duplicados completos
- Estandariza valores categ√≥ricos:
  - 'Si', 'si', 's', '1' ‚Üí 'SI'
  - 'No', 'no', 'n', '0' ‚Üí 'NO'
  - NOTA: Mantiene '99999' como c√≥digo v√°lido del sistema

PASO 5: MANEJO DE VALORES FALTANTES
--------
- Calcula % de nulos por columna
- Genera reporte de nulos en Excel
- Identifica columnas con >90% nulos (casi vac√≠as)
- Imputa valores en categ√≥ricas (<50% nulos):
  - Rellena con 'No especificado'
- Genera gr√°fico de Top 20 variables con m√°s nulos
- NO elimina autom√°ticamente columnas vac√≠as (solo reporta)

PASO 6: AN√ÅLISIS EXPLORATORIO INICIAL (EDA)
--------
- Estad√≠sticas descriptivas de variables num√©ricas:
  - Media, mediana, desviaci√≥n est√°ndar, etc.
  - Exporta a Excel
- Distribuciones de variables categ√≥ricas:
  - Gr√°ficos de barras para primeras 10 variables
  - Solo si tienen <50 categor√≠as √∫nicas
- An√°lisis temporal:
  - Si existe columna A√ëO o FECHA_INTERVENCION
  - Genera gr√°fico de distribuci√≥n anual
  - Crea columna A√ëO si no existe

PASO 7: EXPORTACI√ìN DE BASE FINAL
--------
- Exporta datos limpios a:
  - Excel: base_nna_limpia.xlsx
  - CSV: base_nna_limpia.csv (con UTF-8-BOM)
- Genera resumen final en JSON con:
  - Fecha de procesamiento
  - Dimensiones finales (filas √ó columnas)
  - Tama√±o en MB
  - Nombre del archivo
- Guarda log completo del proceso

============================================================
 ARCHIVOS DE SALIDA
============================================================

 data/processed/
‚îú‚îÄ‚îÄ base_nna_limpia.xlsx          #  Base principal limpia (Excel)
‚îî‚îÄ‚îÄ base_nna_limpia.csv           # Base limpia en CSV

 reports/preparation/
‚îú‚îÄ‚îÄ preparation_log.txt           # Log detallado del proceso
‚îú‚îÄ‚îÄ mapeo_columnas.json           # Mapeo de nombres originales ‚Üí limpios
‚îú‚îÄ‚îÄ reporte_nulos.xlsx            # An√°lisis de valores faltantes
‚îú‚îÄ‚îÄ estadisticas_descriptivas.xlsx # Stats de variables num√©ricas
‚îî‚îÄ‚îÄ resumen_final.json            # Resumen ejecutivo

 reports/preparation/figures/
‚îú‚îÄ‚îÄ valores_faltantes.png         # Top 20 variables con nulos
‚îú‚îÄ‚îÄ distribucion_anual.png        # Intervenciones por a√±o
‚îî‚îÄ‚îÄ dist_*.png                    # Distribuciones de categ√≥ricas

============================================================
 DECISIONES CLAVE DE LIMPIEZA
============================================================
 SE MANTIENEN:
- C√≥digos '99999' (son parte del sistema, NO son missing)
- Columnas NUMERO_DE_MANZANA, NUMERO_DE_FICHA
- Todas las columnas con <90% de nulos

 SE ELIMINAN:
- Informaci√≥n personal: nombres, tel√©fonos, correos, direcciones
- Duplicados completos (filas id√©nticas)
- Caracteres especiales en nombres de columnas

 SE TRANSFORMAN:
- Fechas ‚Üí datetime
- Texto ‚Üí limpio sin espacios extra
- Categor√≠as ‚Üí estandarizadas (SI/NO/99999)
- Columnas ‚Üí MAY√öSCULAS con guiones bajos

============================================================
 CONFIGURACI√ìN
============================================================

Variables de entorno (.env):
- DATA_FILE: Ruta del archivo de datos original

Librer√≠as requeridas:
- pandas, numpy: Manipulaci√≥n de datos
- matplotlib, seaborn: Visualizaci√≥n
- python-dotenv: Variables de entorno
- openpyxl: Lectura/escritura Excel

============================================================
 RESULTADO FINAL
============================================================

Se obtiene una base de datos:
‚úì Sin informaci√≥n personal (RGPD compliant)
‚úì Con columnas estandarizadas
‚úì Sin duplicados
‚úì Con fechas en formato correcto
‚úì Con categor√≠as estandarizadas
‚úì Lista para an√°lisis espacio-temporal

La base limpia queda en:
 data/processed/base_nna_limpia.xlsx

============================================================
"""



# ============================================================
# CONFIGURACI√ìN DE RUTAS
# ============================================================

load_dotenv()

BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_FILE = os.getenv("DATA_FILE")

if not DATA_FILE:
    raise ValueError(" No se encontr√≥ la variable DATA_FILE en el archivo .env")

file_path = os.path.join(BASE_DIR, DATA_FILE)

# Carpetas de salida
PROCESSED_DIR = os.path.join(BASE_DIR, "data", "processed") 
PREP_REPORTS_DIR = os.path.join(BASE_DIR, "reports", "preparation") 
PREP_FIGURES_DIR = os.path.join(PREP_REPORTS_DIR, "figures") 
CLEANED_DATA_FILE = os.path.join(PROCESSED_DIR, "base_nna_limpia.xlsx") 
PREP_LOG_FILE = os.path.join(PREP_REPORTS_DIR, "preparation_log.txt") #

for path in [PROCESSED_DIR, PREP_REPORTS_DIR, PREP_FIGURES_DIR]:
    os.makedirs(path, exist_ok=True)#

# Iniciar log
log_entries = []

def log(message):
    """Registra mensaje en consola y en log"""
    print(message)
    log_entries.append(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")


# ============================================================
# PASO 1: CONFIGURACI√ìN Y CARGA DE DATOS (Integrado con Understanding)
# ============================================================

def paso1_cargar_datos():
    """Carga inicial de datos y sincronizaci√≥n con Data Understanding"""
    log("\n PASO 1: Configuraci√≥n y carga de datos")
    log("-"*70)
    
    ext = os.path.splitext(file_path)[-1].lower()
    log(f" Cargando archivo: {os.path.basename(file_path)}")

    if ext in ['.xlsx', '.xls']:
        xls = pd.ExcelFile(file_path)
        sheet = 'BD' if 'BD' in xls.sheet_names else xls.sheet_names[-1]
        df = pd.read_excel(file_path, sheet_name=sheet)
        log(f"   ‚úì Hoja cargada: {sheet}")
    elif ext == '.csv':
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8')
        log("   ‚úì CSV cargado")
    else:
        raise ValueError("Formato no compatible")

    log(f"   Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
    log(f"   Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

    # ------------------------------------------------------------
    # üîó Integrar configuraci√≥n desde Data Understanding
    # ------------------------------------------------------------
    understanding_config = os.path.join(BASE_DIR, "reports", "understanding", "config_understanding.json")
    global CODIGOS_VALIDOS

    if os.path.exists(understanding_config):
        with open(understanding_config, "r", encoding="utf-8") as f:
            config = json.load(f)
        CODIGOS_VALIDOS = config.get("codigos_validos", ["99999"])
        log(f"   ‚úì Configuraci√≥n importada desde: config_understanding.json")
        log(f"     C√≥digos v√°lidos reconocidos: {CODIGOS_VALIDOS}")
    else:
        CODIGOS_VALIDOS = ["99999"]
        log("   ‚ö†Ô∏è No se encontr√≥ configuraci√≥n de Understanding. Se usar√° 99999 por defecto.")

    return df


# ============================================================
# PASO 2: ESTANDARIZACI√ìN DE NOMBRES DE COLUMNAS
# ============================================================

def paso2_estandarizar_columnas(df): 
    """Limpia y estandariza nombres de columnas""" 
    log("\n  PASO 2: Estandarizaci√≥n de nombres de columnas") 
    log("-"*70) 
    
    original_cols = df.columns.tolist() 
    
    # Limpieza y normalizaci√≥n
    df.columns = (
        df.columns
        .str.strip()                          # Eliminar espacios
        .str.replace(r'\s+', '_', regex=True) # Espacios a guiones bajos
        .str.replace(r'[^\w_]', '', regex=True) # Eliminar caracteres especiales
        .str.upper()                          # Todo en may√∫sculas
    )
    
    # Eliminar guiones bajos duplicados
    df.columns = df.columns.str.replace(r'_+', '_', regex=True).str.strip('_') #
    
    # Mapeo de cambios
    cambios = {orig: nuevo for orig, nuevo in zip(original_cols, df.columns) if orig != nuevo} 
    
    log(f" Columnas estandarizadas: {len(df.columns)} variables")#
    log(f"   Columnas modificadas: {len(cambios)}")
    
    # Guardar mapeo
    with open(os.path.join(PREP_REPORTS_DIR, "mapeo_columnas.json"), "w", encoding="utf-8") as f:
        json.dump(cambios, f, indent=2, ensure_ascii=False) 
    
    return df


# ============================================================
# PASO 3: ELIMINACI√ìN DE INFORMACI√ìN PERSONAL (PII)
# ============================================================

def paso3_eliminar_pii(df):
    """Elimina informaci√≥n personal identificable"""
    log("\n PASO 3: Eliminaci√≥n de informaci√≥n personal (PII)")
    log("-"*70) 
    
    # Columnas con informaci√≥n personal
    pii_patterns = [
        'USUARIO', 'NOMBRE', 'APELLIDO', 'DOCUMENTO', 'CEDULA', 'IDENTIFICACION',
        'DIRECCION', 'DIRECCION', 'DOMICILIO', 'RESIDENCIA',
        'CORREO', 'EMAIL', 'MAIL',
        'TELEFONO', 'CELULAR', 'MOVIL', 'CONTACTO',
        'RESPONSABLE', 'ACUDIENTE', 'TUTOR',
        'NOMBRE_EAPB', 'EPS'
    ]
    
    # Identificar columnas PII
    cols_pii = []
    for col in df.columns:
        if any(pattern in col for pattern in pii_patterns):
            # Excepciones: columnas que contienen "NUMERO" pero no son personales
            if 'NUMERO_DE_MANZANA' in col or 'NUMERO_DE_FICHA' in col:
                continue
            cols_pii.append(col)
    
    # Eliminar columnas PII
    if cols_pii:
        df = df.drop(columns=cols_pii)
        log(f" Columnas PII eliminadas: {len(cols_pii)}")
        for col in cols_pii:
            log(f"   ‚Ä¢ {col}")
    else:
        log(" No se encontraron columnas PII adicionales (archivo ya anonimizado)")
    
    return df


# ============================================================
# PASO 4: Limpieza de inconsistencias (alineado con Data Understanding)
# ============================================================

def paso4_limpiar_inconsistencias(df):
    """Limpieza general sin alterar los c√≥digos v√°lidos (como 99999)"""
    log("\n PASO 4: Limpieza de inconsistencias")
    log("-"*70)

    # Solo operar sobre columnas que son texto (tipo object o string)
    text_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()
    log(f"   Columnas de texto detectadas: {len(text_cols)}")

    for col in text_cols:
        # Limpiar espacios en blanco solo en celdas que son realmente texto
        df[col] = df[col].apply(
            lambda x: re.sub(r'\s+', ' ', x.strip()) if isinstance(x, str) else x
        )
        # Reemplazar cadenas vac√≠as o nulos mal escritos por NaN
        df[col] = df[col].replace(
            ['nan', 'None', 'NULL', '', 'N/A', 'NA', 'n/a', 'na'], np.nan
        )

    # Limpieza de fechas: intentar convertir sin tocar 99999
    date_cols = [c for c in df.columns if 'FECHA' in c.upper()]
    for col in date_cols:
        try:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            log(f"   ‚úì Fecha estandarizada: {col}")
        except Exception:
            log(f"   ‚ö† No se pudo convertir: {col}")

    # Eliminar duplicados completos
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        df = df.drop_duplicates()
        log(f"   ‚úì Duplicados eliminados: {duplicates}")
    else:
        log("   ‚úì No se encontraron duplicados completos")

    # Normalizar variables categ√≥ricas comunes
    cat_estandar = {
        'SI': ['Si', 's√≠', 'S√≠', 'si', 'S', 's', 1],
        'NO': ['No', 'no', 'N', 'n', 0]
    }

    for col in text_cols:
        if df[col].nunique(dropna=True) < 25:
            for val, variants in cat_estandar.items():
                df[col] = df[col].replace(variants, val)

    log("   ‚úì Limpieza de inconsistencias completada correctamente (manteniendo 99999)")
    return df


# ============================================================
# PASO 5: MANEJO DE VALORES FALTANTES (manteniendo 99999)
# ============================================================

def paso5_manejar_faltantes(df):
    """Genera reportes de nulos reales y de c√≥digos v√°lidos (ej. 99999).
    Cuenta tanto la forma num√©rica como la string del c√≥digo v√°lido."""
    log("\n PASO 5: Manejo de valores faltantes")
    log("-"*70)

    tables_dir = os.path.join(PREP_REPORTS_DIR, 'tables')
    figures_dir = os.path.join(PREP_REPORTS_DIR, 'figures')
    os.makedirs(tables_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)

    # --- Definir c√≥digos v√°lidos a considerar (por defecto 99999) ---
    # Si exportaste configuraci√≥n desde data_understanding, CODIGOS_VALIDOS ya deber√≠a existir.
    try:
        codigos = CODIGOS_VALIDOS
    except NameError:
        codigos = ["99999"]

    # --- Crear una copia temporal donde reemplazamos los c√≥digos v√°lidos por NaN
    temp = df.copy()

    for code in codigos:
        # reemplazar tanto la forma num√©rica como la string
        temp.replace([code, str(code)], np.nan, inplace=True)

    # --- Reporte de nulos reales (ahora sin contar c√≥digos v√°lidos) ---
    missing_report = temp.isnull().sum().reset_index()
    missing_report.columns = ['Variable', 'Nulos']
    missing_report['Porcentaje'] = (missing_report['Nulos'] / len(df) * 100).round(6)
    missing_report = missing_report.sort_values(by='Porcentaje', ascending=False)
    missing_report.to_excel(os.path.join(tables_dir, 'reporte_nulos.xlsx'), index=False)
    log("   ‚úì Reporte de nulos reales generado (reporte_nulos.xlsx)")

    # Columnas con >90% de nulos reales
    high_missing = missing_report[missing_report['Porcentaje'] > 90]
    if not high_missing.empty:
        log(f"     Columnas con >90% nulos reales: {len(high_missing)}")
        log(f"   Considerar: {', '.join(high_missing['Variable'].tolist()[:10])}...")

    # --- Reporte de frecuencias para cada c√≥digo v√°lido (ej. 99999) ---
    for code in codigos:
        freq_series = df.apply(lambda col: ((col == code) | (col == str(code))).sum())
        report_code = freq_series.reset_index()
        report_code.columns = ['Variable', f'Frecuencia_{code}']
        report_code = report_code[report_code[f'Frecuencia_{code}'] > 0].sort_values(by=f'Frecuencia_{code}', ascending=False)
        out_path = os.path.join(tables_dir, f'reporte_{code}.xlsx')
        report_code.to_excel(out_path, index=False)
        log(f"   ‚úì Reporte de frecuencias '{code}' generado ({out_path})")

    # --- Resumen global de faltantes reales ---
    total_missing = int(missing_report['Nulos'].sum())
    missing_pct = (total_missing / (len(df) * len(df.columns))) * 100
    log(f"   Total de valores faltantes reales en la base: {total_missing:,}")
    log(f"   Porcentaje global de faltantes reales: {missing_pct:.2f}%")

    # --- Gr√°fico (distribuci√≥n de % nulos reales) ---
    plt.figure(figsize=(10, 5))
    sns.histplot(missing_report['Porcentaje'].clip(0,100), bins=30)
    plt.title("Distribuci√≥n de % de valores faltantes reales (sin c√≥digos v√°lidos)")
    plt.xlabel("% valores faltantes reales")
    plt.ylabel("Frecuencia")
    plt.tight_layout()
    plt.savefig(os.path.join(figures_dir, 'faltantes_distribucion.png'))
    plt.close()
    log("   ‚úì Gr√°fico de distribuci√≥n de nulos guardado")

    return df
# ============================================================
# PASO 6: AN√ÅLISIS EXPLORATORIO INICIAL (EDA)
# ============================================================

def paso6_eda_inicial(df):
    """An√°lisis exploratorio de datos limpiados"""
    log("\n PASO 6: An√°lisis exploratorio inicial (EDA)")
    log("-"*70)
    
    # Resumen estad√≠stico de num√©ricas
    num_cols = df.select_dtypes(include=[np.number]).columns
    if len(num_cols) > 0:
        stats = df[num_cols].describe().T
        stats.to_excel(os.path.join(PREP_REPORTS_DIR, "estadisticas_descriptivas.xlsx"))
        log(f"   ‚úì Estad√≠sticas descriptivas guardadas ({len(num_cols)} variables num√©ricas)")
    
    # Distribuci√≥n de variables categ√≥ricas clave
    cat_cols = df.select_dtypes(include=['object']).columns[:10]  # Primeras 10
    
    for col in cat_cols:
        if df[col].nunique() < 50:  # Solo si tiene pocas categor√≠as
            freq = df[col].value_counts().head(15)
            
            plt.figure(figsize=(10, 6))
            freq.plot(kind='barh', color='steelblue')
            plt.title(f"Distribuci√≥n de {col}")
            plt.xlabel("Frecuencia")
            plt.tight_layout()
            plt.savefig(os.path.join(PREP_FIGURES_DIR, f"dist_{col}.png"), dpi=150)
            plt.close()
    
    log(f"   ‚úì Distribuciones generadas para variables categ√≥ricas")
    
    # An√°lisis temporal si existe columna de a√±o
    if 'A√ëO' in df.columns or 'FECHA_INTERVENCION' in df.columns:
        if 'A√ëO' not in df.columns and 'FECHA_INTERVENCION' in df.columns:
            df['A√ëO'] = pd.to_datetime(df['FECHA_INTERVENCION']).dt.year
        
        if 'A√ëO' in df.columns:
            dist_anual = df['A√ëO'].value_counts().sort_index()
            
            plt.figure(figsize=(10, 6))
            dist_anual.plot(kind='bar', color='mediumseagreen')
            plt.title("Distribuci√≥n de intervenciones por a√±o")
            plt.xlabel("A√±o")
            plt.ylabel("N√∫mero de intervenciones")
            plt.xticks(rotation=0)
            plt.tight_layout()
            plt.savefig(os.path.join(PREP_FIGURES_DIR, "distribucion_anual.png"), dpi=150)
            plt.close()
            log(f"   ‚úì Distribuci√≥n temporal generada")
    
    log(f" An√°lisis exploratorio inicial completado (sin excluir '99999')")
    
    return df


# ============================================================
# PASO 7: EXPORTACI√ìN DE BASE FINAL
# ============================================================

def paso7_exportar_datos(df):
    """Exporta base de datos limpia"""
    log("\n PASO 7: Exportaci√≥n de la base final")
    log("-"*70)
    
    # Exportar a Excel
    df.to_excel(CLEANED_DATA_FILE, index=False, engine='openpyxl')
    log(f" Base limpia exportada a: {CLEANED_DATA_FILE}")
    
    # Tambi√©n en CSV
    csv_file = CLEANED_DATA_FILE.replace('.xlsx', '.csv')
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    log(f" Base limpia exportada a: {csv_file}")
    
    # Generar resumen final
    resumen = {
        "fecha_procesamiento": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "filas_finales": int(df.shape[0]),
        "columnas_finales": int(df.shape[1]),
        "archivo_salida": os.path.basename(CLEANED_DATA_FILE),
        "tama√±o_mb": float((df.memory_usage(deep=True).sum() / 1024**2).round(2))
    }
    
    with open(os.path.join(PREP_REPORTS_DIR, "resumen_final.json"), "w", encoding="utf-8") as f:
        json.dump(resumen, f, indent=2, ensure_ascii=False)
    
    log(f" Resumen final generado")
    
    return df


# ============================================================
# FUNCI√ìN PRINCIPAL
# ============================================================

def main():
    """Ejecuta todo el proceso de preparaci√≥n"""
    inicio = datetime.now()
    
    try:
        # Ejecutar pasos
        df = paso1_cargar_datos()
        df = paso2_estandarizar_columnas(df)
        df = paso3_eliminar_pii(df)
        df = paso4_limpiar_inconsistencias(df)
        df = paso5_manejar_faltantes(df)
        df = paso6_eda_inicial(df)
        df = paso7_exportar_datos(df)
        
        # Resumen final
        duracion = (datetime.now() - inicio).total_seconds() 
        
        log("\n" + "="*70)
        log(" DATA PREPARATION COMPLETADO CON √âXITO")
        log("="*70)
        log(f"\n Resumen final:")
        log(f"   ‚Ä¢ Filas en base limpia: {df.shape[0]:,}")
        log(f"   ‚Ä¢ Columnas en base limpia: {df.shape[1]}")
        log(f"   ‚Ä¢ Tiempo de ejecuci√≥n: {duracion:.2f} segundos")
        log(f"\n Archivos generados:")
        log(f"   ‚Ä¢ Base limpia: {CLEANED_DATA_FILE}")
        log(f"   ‚Ä¢ Reportes: {PREP_REPORTS_DIR}")
        log(f"   ‚Ä¢ Figuras: {PREP_FIGURES_DIR}")
        log("\n" + "="*70)
        
    except Exception as e:
        log(f"\n ERROR: {str(e)}") 
        import traceback
        traceback.print_exc()
    
    finally:
        # Guardar log
        with open(PREP_LOG_FILE, "w", encoding="utf-8") as f:
            f.write("\n".join(log_entries))
        log(f"\nüìÑ Log guardado en: {PREP_LOG_FILE}") 


if __name__ == "__main__":
    main()

# ============================================================
# FIN DEL SCRIPT
# ============================================================