# ============================================================
# DATA UNDERSTANDING - Proyecto NNA Bogot√° (2021‚Äì2025)
# Versi√≥n extendida con an√°lisis espacio-temporal
# ============================================================

import os # manejo de rutas y directorios.
import pandas as pd # manipulaci√≥n de datos.
import numpy as np # manipulaci√≥n num√©rica.
import hashlib # funciones hash para anonimizaci√≥n.
import matplotlib.pyplot as plt # visualizaci√≥n.
import seaborn as sns # visualizaci√≥n avanzada.
from dotenv import load_dotenv # carga de variables de entorno.
from datetime import datetime # manejo de fechas.
import warnings # manejo de advertencias.
import json # manejo de archivos JSON.

warnings.filterwarnings('ignore')# Ignorar advertencias innecesarias
sns.set_style("whitegrid")# Estilo de gr√°ficos


"""
============================================================
RESUMEN EJECUTIVO DEL SCRIPT - DATA UNDERSTANDING
Proyecto: NNA Bogot√° (2021-2025)
============================================================

 PROP√ìSITO GENERAL:
Este script automatiza la fase de "Data Understanding" (Comprensi√≥n de Datos) 
de la metodolog√≠a CRISP-DM para analizar intervenciones con Ni√±os, Ni√±as y 
Adolescentes en Bogot√° durante 2021-2025.

 OBJETIVOS PRINCIPALES:
1. Cargar y explorar datos de intervenciones con NNA
2. Analizar tendencias espacio-temporales por localidad
3. Identificar zonas de alerta con cambios significativos (>20%)
4. Estudiar la relaci√≥n con el r√©gimen de afiliaci√≥n en salud (subsidiado)
5. Generar reportes autom√°ticos y visualizaciones

============================================================
 ESTRUCTURA DEL SCRIPT (11 PASOS)
============================================================

1Ô∏è  CONFIGURACI√ìN INICIAL
   ‚Ä¢ Carga variables de entorno (.env)
   ‚Ä¢ Define rutas de entrada/salida
   ‚Ä¢ Crea estructura de carpetas autom√°ticamente

2Ô∏è  CARGA DE DATOS
   ‚Ä¢ Lee archivos Excel (.xlsx, .xls) o CSV
   ‚Ä¢ Detecta autom√°ticamente hojas y separadores
   ‚Ä¢ Muestra dimensiones del dataset

3Ô∏è  LIMPIEZA Y NORMALIZACI√ìN
   ‚Ä¢ Estandariza nombres de columnas (may√∫sculas, sin espacios)
   ‚Ä¢ Verifica anonimizaci√≥n (elimina datos personales si existen)
   ‚Ä¢ Guarda mapeo de nombres originales ‚Üí normalizados

4Ô∏è  DETECCI√ìN TEMPORAL
   ‚Ä¢ Identifica columnas de fecha/a√±o
   ‚Ä¢ Extrae a√±o de fechas autom√°ticamente
   ‚Ä¢ Detecta rango temporal disponible (2021-2025)

5Ô∏è  DICCIONARIO DE DATOS
   ‚Ä¢ Documenta todas las variables del dataset
   ‚Ä¢ Clasifica por tipo: num√©ricas, categ√≥ricas, identificadores, etc.
   ‚Ä¢ Calcula: nulos, √∫nicos, cardinalidad, primer valor
   ‚Ä¢ Exporta a Excel y CSV

6Ô∏è  VERIFICACI√ìN DE CALIDAD
   ‚Ä¢ Detecta duplicados completos
   ‚Ä¢ Identifica columnas constantes o casi vac√≠as (>90% nulos)
   ‚Ä¢ Encuentra columnas con alta cardinalidad
   ‚Ä¢ Calcula m√©tricas: % nulos, memoria usada
   ‚Ä¢ Genera reporte JSON de calidad

7Ô∏è AN√ÅLISIS EXPLORATORIO
   ‚Ä¢ Frecuencias de variables categ√≥ricas (Top 20)
   ‚Ä¢ Estad√≠sticas descriptivas de num√©ricas
   ‚Ä¢ Genera gr√°ficos de barras e histogramas
   ‚Ä¢ Limita a primeras 15 categ√≥ricas y 10 num√©ricas
8Ô∏è  AN√ÅLISIS ESPACIO-TEMPORAL  (NUEVO - Objetivo principal)
   ‚Ä¢ Agrupa intervenciones por localidad y a√±o
   ‚Ä¢ Calcula tendencias: aumento/disminuci√≥n por localidad
   ‚Ä¢ Identifica zonas de alerta con cambios >20%
   ‚Ä¢ Genera:
     - Tabla pivote localidad √ó a√±o
     - Gr√°fico de evoluci√≥n temporal (Top 10 localidades)
     - Mapa de calor de intervenciones

9Ô∏è  AN√ÅLISIS R√âGIMEN DE SALUD (NUEVO - Objetivo espec√≠fico 3)
   ‚Ä¢ Distribuye intervenciones por tipo de afiliaci√≥n (subsidiado, contributivo)
   ‚Ä¢ Cruza localidad √ó r√©gimen de salud (porcentajes)
   ‚Ä¢ Identifica Top 10 localidades con mayor % subsidiado
   ‚Ä¢ Genera gr√°ficos:
     - Distribuci√≥n general de reg√≠menes
     - Barras apiladas por localidad

10 CRUCES DE VARIABLES
   ‚Ä¢ Localidad √ó Tipo de intervenci√≥n
   ‚Ä¢ A√±o √ó Tipo de intervenci√≥n
   ‚Ä¢ Motivo √ó Servicio (si existen)
   ‚Ä¢ Exporta matrices y mapas de calor

10.1 VISUALIZACIONES GENERALES
   ‚Ä¢ Valores faltantes (Top 20)
   ‚Ä¢ Matriz de correlaci√≥n de variables num√©ricas

1Ô∏è0.2 DOCUMENTACI√ìN MARKDOWN
   ‚Ä¢ Genera reporte completo siguiendo CRISP-DM:
     - 2.1 Collect Initial Data
     - 2.2 Describe Data
     - 2.3 Explore Data
     - 2.4 Verify Data Quality
   ‚Ä¢ Incluye tablas de zonas de alerta
   ‚Ä¢ Lista archivos generados

============================================================
 ARCHIVOS DE SALIDA GENERADOS
============================================================

 reports/tables/
‚îú‚îÄ‚îÄ diccionario_datos.xlsx        # Documentaci√≥n completa de variables
‚îú‚îÄ‚îÄ column_mapping.json            # Mapeo de nombres originales
‚îú‚îÄ‚îÄ quality_report.json            # Reporte de calidad JSON
‚îú‚îÄ‚îÄ quality_summary.csv            # Resumen de calidad CSV
‚îú‚îÄ‚îÄ distribucion_localidad_a√±o.csv # Datos espacio-temporales
‚îú‚îÄ‚îÄ matriz_localidad_a√±o.xlsx     # Tabla pivote
‚îú‚îÄ‚îÄ tendencias_por_localidad.xlsx # Cambios y tendencias
‚îú‚îÄ‚îÄ zonas_alerta.csv              #  Localidades con cambios >20%
‚îú‚îÄ‚îÄ distribucion_regimen_salud.csv
‚îú‚îÄ‚îÄ regimen_por_localidad_porcentaje.xlsx
‚îú‚îÄ‚îÄ top_localidades_subsidiado.csv
‚îú‚îÄ‚îÄ cruce_localidad_tipo_intervencion.xlsx
‚îú‚îÄ‚îÄ cruce_a√±o_tipo_intervencion.xlsx
‚îú‚îÄ‚îÄ estadisticas_numericas.xlsx
‚îî‚îÄ‚îÄ frecuencia_*.csv (m√∫ltiples)  # Frecuencias por variable

 reports/figures/temporal/
‚îî‚îÄ‚îÄ evolucion_temporal_top10.png  # Serie temporal por localidad

 reports/figures/spatial/
‚îú‚îÄ‚îÄ heatmap_localidad_a√±o.png     # Mapa de calor
‚îú‚îÄ‚îÄ top_subsidiado_localidades.png
‚îî‚îÄ‚îÄ regimen_por_localidad_stacked.png

 reports/figures/exploratory/
‚îú‚îÄ‚îÄ missing_values.png             # Valores faltantes
‚îú‚îÄ‚îÄ correlation_matrix.png         # Correlaciones
‚îú‚îÄ‚îÄ distribucion_regimen.png
‚îú‚îÄ‚îÄ cruce_localidad_tipo.png
‚îú‚îÄ‚îÄ dist_*.png (m√∫ltiples)        # Distribuciones categ√≥ricas
‚îî‚îÄ‚îÄ hist_*.png (m√∫ltiples)        # Histogramas num√©ricas

 reports/
‚îî‚îÄ‚îÄ data_summary.md                #  Reporte Markdown completo

============================================================
 FUNCIONES PRINCIPALES
============================================================

load_data()              ‚Üí Carga Excel/CSV con detecci√≥n autom√°tica
clean_columns()          ‚Üí Normaliza nombres de columnas
anonymize()              ‚Üí Verifica y elimina datos personales
detect_temporal_columns()‚Üí Detecta y extrae variables temporales
generate_dictionary()    ‚Üí Genera diccionario de datos
clasificar_variable()    ‚Üí Clasifica variables por tipo
verify_quality()         ‚Üí Verifica calidad del dataset
exploratory_analysis()   ‚Üí EDA de categ√≥ricas y num√©ricas
analyze_spatiotemporal() ‚Üí An√°lisis espacio-temporal (CORE)
analyze_health_regime()  ‚Üí An√°lisis r√©gimen subsidiado
cross_analysis()         ‚Üí Cruces de variables clave
plot_missing()           ‚Üí Gr√°fico de valores faltantes
plot_correlation()       ‚Üí Matriz de correlaci√≥n
generate_summary_md()    ‚Üí Genera reporte Markdown
main()                   ‚Üí Flujo principal (orquestador)

============================================================
 CONFIGURACI√ìN REQUERIDA
============================================================

1. Archivo .env con:
   DATA_FILE=data/raw/base_datos_completa_NNA_TI_anon.xlsx

2. Librer√≠as necesarias:
   - pandas, numpy (manipulaci√≥n de datos)
   - matplotlib, seaborn (visualizaci√≥n)
   - python-dotenv (variables de entorno)
   - openpyxl (lectura/escritura Excel)

3. Estructura de carpetas (se crea autom√°ticamente):
   - data/raw/ (entrada)
   - reports/tables/ (tablas)
   - reports/figures/ (gr√°ficos)

============================================================
 CARACTER√çSTICAS ESPECIALES
============================================================

Detecci√≥n autom√°tica de formatos (Excel/CSV)
Manejo de datos ya anonimizados (no re-encripta)
Clasificaci√≥n inteligente de variables
Identificaci√≥n autom√°tica de zonas de alerta
An√°lisis temporal con detecci√≥n de tendencias
Cruces autom√°ticos de variables relacionadas
Exportaci√≥n m√∫ltiple (Excel, CSV, JSON, PNG, MD)
Reportes siguiendo metodolog√≠a CRISP-DM
Manejo robusto de errores
Log completo en consola

============================================================
 INDICADORES CLAVE CALCULADOS
============================================================

- Cambio absoluto: diferencia entre a√±o final e inicial
- Cambio porcentual: ((final - inicial) / (inicial + 1)) * 100
- Tendencia: clasificaci√≥n cualitativa del cambio
  - Aumento fuerte: >20%
  - Aumento moderado: 5-20%
  - Estable: -5% a 5%
  - Disminuci√≥n moderada: -20% a -5%
  - Disminuci√≥n fuerte: <-20%

- Zonas de alerta: localidades con |cambio| > 20%

============================================================
 RESPONDE A LOS OBJETIVOS DEL PROYECTO
============================================================

Objetivo 1:  Describir distribuci√≥n por localidad y a√±o
Objetivo 2:  Identificar localidades con aumento/disminuci√≥n
Objetivo 3:  Relacionar con r√©gimen de afiliaci√≥n (subsidiado)

Preguntas gu√≠a:
 ¬øD√≥nde aumentaron/disminuyeron las intervenciones?
 ¬øQu√© caracter√≠sticas tienen las localidades con mayor crecimiento?
 ¬øExiste relaci√≥n entre intervenciones y r√©gimen subsidiado?


============================================================
 AUTOR Y VERSI√ìN
============================================================

Proyecto: An√°lisis NNA Bogot√° (2021-2025)
Metodolog√≠a: CRISP-DM
Fase: Data Understanding
Versi√≥n: 2.0 (con an√°lisis espacio-temporal).Karen Suarez
Fecha: Octubre 2025

============================================================
"""

# ============================================================
# 1. CONFIGURACI√ìN DE RUTAS
# ============================================================

load_dotenv() # Cargar variables de entorno desde .env

BASE_DIR = os.path.dirname(os.path.dirname(__file__)) # Directorio base del proyecto
DATA_FILE = os.getenv("DATA_FILE")# Ruta del archivo de datos desde .env

if not DATA_FILE: # Verificar que la variable exista
    raise ValueError(" No se encontr√≥ la variable DATA_FILE en el archivo .env") #  Error si no existe

file_path = os.path.join(BASE_DIR, DATA_FILE)# Ruta completa del archivo de datos

if not os.path.exists(file_path):# Verificar que el archivo exista
    raise FileNotFoundError(f"‚ùå No se encontr√≥ el archivo: {file_path}")# Error si no existe

# Estructura de carpetas
REPORTS_DIR = os.path.join(BASE_DIR, "reports")# Carpeta principal de reportes
FIGURES_DIR = os.path.join(REPORTS_DIR, "figures")# Carpeta de figuras
TABLES_DIR = os.path.join(REPORTS_DIR, "tables")# Carpeta de tablas
TEMPORAL_DIR = os.path.join(FIGURES_DIR, "temporal")# Carpeta de figuras temporales
SPATIAL_DIR = os.path.join(FIGURES_DIR, "spatial")# Carpeta de figuras espaciales
EXPLORATORY_DIR = os.path.join(FIGURES_DIR, "exploratory")# Carpeta de figuras exploratorias
SUMMARY_FILE = os.path.join(REPORTS_DIR, "data_summary.md")# Resumen en Markdown
ALERT_ZONES_FILE = os.path.join(TABLES_DIR, "zonas_alerta.csv")# Zonas de alerta

for path in [REPORTS_DIR, FIGURES_DIR, TABLES_DIR, TEMPORAL_DIR, SPATIAL_DIR, EXPLORATORY_DIR]:# Crear carpetas si no existen
    os.makedirs(path, exist_ok=True)# Crear carpetas si no existen

print(f" Estructura de carpetas creada en: {REPORTS_DIR}")    


# ============================================================
# 2. CARGA DE DATOS CON DETECCI√ìN AUTOM√ÅTICA
# ============================================================

def load_data(file_path):
    """Carga datos desde Excel o CSV con detecci√≥n autom√°tica""" # Carga datos desde Excel o CSV con detecci√≥n autom√°tica
    ext = os.path.splitext(file_path)[-1].lower() # Extensi√≥n del archivo
    
    print(f" Cargando archivo: {os.path.basename(file_path)}") # Cargando archivo
    
    if ext in ['.xlsx', '.xls']:# Archivo Excel
        xls = pd.ExcelFile(file_path)# Cargar archivo Excel
        print(f"   Hojas disponibles: {xls.sheet_names}")# Listar hojas disponibles
        sheet = 'BD' if 'BD' in xls.sheet_names else xls.sheet_names[-1]# Seleccionar hoja 'BD' 
        df = pd.read_excel(file_path, sheet_name=sheet)# Cargar hoja seleccionada
        print(f"   ‚úì Cargada hoja: {sheet}")# Confirmar hoja cargada
    elif ext == '.csv':
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8')# Cargar CSV con detecci√≥n autom√°tica de separador
        print(f"   ‚úì CSV cargado con detecci√≥n autom√°tica de separador")# Confirmar carga CSV
    else:
        raise ValueError("‚ùå Formato no compatible. Usa .csv, .xlsx o .xls")# Error si formato no compatible
    
    print(f"‚úÖ Datos cargados: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")# Confirmar dimensiones
    return df# Retornar DataFrame


# ============================================================
# 3. LIMPIEZA Y NORMALIZACI√ìN
# ============================================================

def clean_columns(df):
    """Limpia y normaliza nombres de columnas"""
    original_cols = df.columns.tolist()# Guardar nombres originales de columnas 
    
    df.columns = (
        df.columns
        .str.strip()
        .str.replace(' ', '_')
        .str.replace(r'[^\w]', '', regex=True)
        .str.upper()
    )# Normalizar nombres de columnas 
    
    col_mapping = dict(zip(original_cols, df.columns))# Mapeo de nombres originales a normalizados
    with open(os.path.join(TABLES_DIR, "column_mapping.json"), "w", encoding="utf-8") as f: # Guardar mapeo en JSON 
        json.dump(col_mapping, f, indent=2, ensure_ascii=False) # Guardar mapeo en JSON 
    
    print(f" Columnas normalizadas: {len(df.columns)} variables")# Confirmar normalizaci√≥n
    return df # Retornar DataFrame


def anonymize(df):
    """Anonimiza datos personales mediante hash irreversible"""
    import hashlib
    
    sensitive_cols = {
        'telefono': ['TEL√âFONO', 'TELEFONO', 'TEL'],
        'correo': ['CORREO', 'EMAIL', 'MAIL'],
        'direccion': ['DIRECCI√ìN', 'DIRECCION', 'DIR'],
        'nombre_eapb': ['NOMBRE_EAPB', 'EAPB'],
        'barrio': ['BARRIO', 'VEREDA']  # Solo si es muy espec√≠fico
    }
    
    def hash_value(value):
        """Genera hash SHA256 irreversible"""
        if pd.isna(value) or value == '99999':
            return value
        return hashlib.sha256(str(value).encode()).hexdigest()[:16]
    
    anonimizadas = []
    
    for categoria, keywords in sensitive_cols.items():
        for col in df.columns:
            if any(kw in col.upper() for kw in keywords):
                if col in df.columns:
                    df[col] = df[col].apply(hash_value)
                    anonimizadas.append(col)
                    print(f"   ‚úì Anonimizada: {col}")
    
    # ELIMINAR columnas de acudiente si existen
    acudiente_cols = [c for c in df.columns if 'ACUDIENTE' in c.upper() and 'NOMBRE' in c.upper()]
    if acudiente_cols:
        df.drop(columns=acudiente_cols, inplace=True)
        print(f"    Eliminadas: {acudiente_cols}")
    
    print(f"\n Anonimizaci√≥n completada: {len(anonimizadas)} columnas procesadas")
    return df


# ============================================================
# 4. DETECCI√ìN AUTOM√ÅTICA DE VARIABLES TEMPORALES
# ============================================================

def detect_temporal_columns(df): 
    """Detecta y procesa columnas de fecha/a√±o (sin tratar 99999 como fecha)."""
    temporal_info = {}
    date_candidates = [c for c in df.columns if any(x in c.upper() for x in ['FECHA', 'DATE', 'A√ëO', 'ANO', 'YEAR', 'MES', 'MONTH'])]

    for col in date_candidates:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            temporal_info[col] = 'datetime'
        elif df[col].dtype == 'object':
            try:
                # No intentar convertir valores tipo '99999'
                df[col] = pd.to_datetime(df[col].replace('99999', np.nan), errors='coerce')
                temporal_info[col] = 'datetime_converted'
            except:
                pass

    # Extraer a√±o si hay columnas de fecha v√°lidas
    for col in temporal_info.keys():
        if 'A√ëO' not in df.columns and 'YEAR' not in df.columns:
            df['A√ëO'] = df[col].dt.year
            print(f"   ‚úì Columna 'A√ëO' extra√≠da de {col}")

    # Validar rango de a√±os 2021‚Äì2025
    if 'A√ëO' in df.columns:
        df['A√ëO'] = pd.to_numeric(df['A√ëO'], errors='coerce')
        df.loc[(df['A√ëO'] < 2021) | (df['A√ëO'] > 2025), 'A√ëO'] = np.nan
        years = df['A√ëO'].dropna().unique()
        print(f" A√±os detectados v√°lidos: {sorted([int(y) for y in years])}")
        temporal_info['years_available'] = sorted([int(y) for y in years])
    else:
        print("  No se detect√≥ columna de a√±o v√°lida")

    return df, temporal_info


# ============================================================
# 5. DICCIONARIO DE DATOS AMPLIADO (Versi√≥n ajustada a NNA)
# ============================================================

def clasificar_variable(row, total_rows):
    """
    Clasifica una variable seg√∫n su tipo y cardinalidad.
    """
    tipo = str(row['Tipo_dato']).lower()
    cardinalidad = row['Cardinalidad']
    n_unicos = row['Valores_√∫nicos']
    nombre = str(row['Variable']).upper()

    # Identificadores t√≠picos
    if 'ID' in nombre or 'IDENTIFICADOR' in nombre or 'DOCUMENTO' in nombre:
        return 'Identificador'
    # Fechas
    if 'FECHA' in nombre or 'DATE' in nombre or 'A√ëO' in nombre or 'YEAR' in nombre or 'MES' in nombre or 'MONTH' in nombre:
        return 'Temporal'
    # Num√©ricas
    if tipo in ['int64', 'float64', 'int32', 'float32']:
        if n_unicos < 10:
            return 'Num√©rica discreta'
        else:
            return 'Num√©rica continua'
    # Categ√≥ricas
    if tipo in ['object', 'category']:
        if n_unicos < 30:
            return 'Categ√≥rica'
        elif cardinalidad > 80:
            return 'Texto libre / alta cardinalidad'
        else:
            return 'Categ√≥rica'
    # Booleanas
    if tipo in ['bool']:
        return 'Booleana'
    return 'Otro'

def generate_dictionary(df):
    """Genera diccionario completo de datos sin tratar '99999' como nulo."""

    # --- Solo cuentan como nulos los valores verdaderamente faltantes ---
    def contar_nulos_reales(col):
        """Cuenta √∫nicamente los NaN reales."""
        return col.isna().sum()

    # --- Limpieza del primer valor mostrado ---
    def limpiar_valor(v):
        """Devuelve None solo si es NaN, deja '99999' tal cual."""
        if pd.isna(v):
            return None
        return str(v).strip()

    # --- Calcular nulos y estad√≠sticas b√°sicas ---
    valores_nulos = [contar_nulos_reales(df[col]) for col in df.columns]

    dic = pd.DataFrame({
        "Variable": df.columns,
        "Tipo_dato": df.dtypes.values,
        "Valores_nulos": valores_nulos,
        "Porcentaje_nulos": (np.array(valores_nulos) / len(df) * 100).round(2),
        "Valores_√∫nicos": df.nunique().values,
        "Cardinalidad": (df.nunique().values / len(df) * 100).round(2),
        "Primer_valor": [limpiar_valor(df[col].dropna().iloc[0]) if len(df[col].dropna()) > 0 else None for col in df.columns]
    })

    # Clasificaci√≥n de variables
    dic['Clasificaci√≥n'] = dic.apply(lambda row: clasificar_variable(row, len(df)), axis=1)

    # Guardar resultados
    dic.to_excel(os.path.join(TABLES_DIR, "diccionario_datos.xlsx"), index=False)
    dic.to_csv(os.path.join(TABLES_DIR, "diccionario_datos.csv"), index=False)

    print(f"üìò Diccionario de datos generado correctamente ({len(dic)} variables).")
    print("   '99999' se considera un valor v√°lido (No Aplica), no un dato faltante.")
    return dic


# ============================================================
# 6. VERIFICACI√ìN DE CALIDAD DE DATOS
# ============================================================

# ============================================================
# 6. VERIFICACI√ìN DE CALIDAD DE DATOS (versi√≥n sin excluir ‚Äú99999‚Äù)
# ============================================================

def verify_quality(df):
    """An√°lisis exhaustivo de calidad de datos sin tratar '99999' como nulo,
    y genera reportes separados para valores nulos y valores '99999'."""
    
    df_check = df.copy()

    def es_nulo_real(x):
        return pd.isna(x)

    # --- M√©tricas generales ---
    total_filas = len(df_check)
    total_columnas = df_check.shape[1]
    filas_duplicadas = df_check.duplicated().sum()
    porcentaje_duplicados = round((filas_duplicadas / total_filas) * 100, 2)
    promedio_nulos = round(
        np.mean([col.map(es_nulo_real).mean() for _, col in df_check.items()]) * 100, 2
    )
    columnas_constantes = len([c for c in df_check.columns if df_check[c].nunique() == 1])
    columnas_casi_vacias = len(
        [c for c in df_check.columns if df_check[c].map(es_nulo_real).mean() > 0.9]
    )
    memoria_mb = round(df_check.memory_usage(deep=True).sum() / 1024**2, 2)

    # --- Columnas problem√°ticas ---
    problematic = []
    for col in df_check.columns:
        issues = []
        if df_check[col].nunique() == 1:
            issues.append("constante")
        if df_check[col].map(es_nulo_real).mean() > 0.9:
            issues.append("casi_vac√≠a")
        if df_check[col].nunique() > len(df_check) * 0.95 and df_check[col].dtype == 'object':
            issues.append("alta_cardinalidad")
        if issues:
            problematic.append({"columna": col, "problemas": issues})

    # --- üîπ NUEVO: reporte de nulos reales ---
    nulos_df = pd.DataFrame({
        "Variable": df_check.columns,
        "Valores_nulos_reales": [df_check[c].isna().sum() for c in df_check.columns],
        "Porcentaje_nulos_reales": [
            (df_check[c].isna().sum() / total_filas * 100).round(2) for c in df_check.columns
        ]
    })
    nulos_df = nulos_df[nulos_df["Valores_nulos_reales"] > 0]
    nulos_path = os.path.join(TABLES_DIR, "reporte_nulos.xlsx")
    nulos_df.to_excel(nulos_path, index=False)

    # --- üîπ NUEVO: reporte de valores '99999' ---
    rep_99999 = pd.DataFrame({
        "Variable": df_check.columns,
        "Cantidad_99999": [(df_check[c] == 99999).sum() + (df_check[c] == '99999').sum() for c in df_check.columns],
        "Porcentaje_99999": [
            (((df_check[c] == 99999).sum() + (df_check[c] == '99999').sum()) / total_filas * 100).round(2)
            for c in df_check.columns
        ]
    })
    rep_99999 = rep_99999[rep_99999["Cantidad_99999"] > 0]
    rep_99999_path = os.path.join(TABLES_DIR, "reporte_99999.xlsx")
    rep_99999.to_excel(rep_99999_path, index=False)

    # --- Guardar resumen general ---
    quality = {
        "total_filas": int(total_filas),
        "total_columnas": int(total_columnas),
        "filas_duplicadas": int(filas_duplicadas),
        "porcentaje_duplicados": float(porcentaje_duplicados),
        "promedio_nulos": float(promedio_nulos),
        "columnas_constantes": int(columnas_constantes),
        "columnas_casi_vacias": int(columnas_casi_vacias),
        "memoria_mb": float(memoria_mb),
        "columnas_problem√°ticas": problematic
    }

    with open(os.path.join(TABLES_DIR, "quality_report.json"), "w", encoding="utf-8") as f:
        json.dump(quality, f, indent=2, ensure_ascii=False)

    pd.Series({k: v for k, v in quality.items() if not isinstance(v, list)}).to_csv(
        os.path.join(TABLES_DIR, "quality_summary.csv")
    )

    print(f" Calidad verificada: {quality['filas_duplicadas']} duplicados, {quality['columnas_constantes']} columnas constantes")
    print("   Nota: los valores '99999' se mantienen como v√°lidos (no se cuentan como nulos).")
    print(f"   Se generaron archivos:\n    ‚Ä¢ {nulos_path}\n    ‚Ä¢ {rep_99999_path}")
    return quality


# ============================================================
# 7. AN√ÅLISIS EXPLORATORIO AMPLIADO
# ============================================================

def exploratory_analysis(df, dic):
    """An√°lisis exploratorio completo de todas las variables"""
    print("\n Iniciando an√°lisis exploratorio ampliado...") # Iniciando an√°lisis exploratorio ampliado
    
    cat_vars = dic[dic['Clasificaci√≥n'].str.contains('Categ√≥rica', na=False)]['Variable'].tolist() # Variables categ√≥ricas
    
    for var in cat_vars[:15]:
        if var in df.columns:
            freq = df[var].value_counts().head(20) # Frecuencia de valores
            freq.to_csv(os.path.join(TABLES_DIR, f"frecuencia_{var}.csv")) # Guardar frecuencia en CSV
            
            if len(freq) > 0:
                plt.figure(figsize=(10, 6)) # Gr√°fico de barras horizontales
                freq.plot(kind='barh', color='steelblue') # Gr√°fico de barras horizontales
                plt.title(f"Distribuci√≥n de {var}")# T√≠tulo del gr√°fico
                plt.xlabel("Frecuencia")# Etiqueta del eje X
                plt.tight_layout()# Ajuste del dise√±o
                plt.savefig(os.path.join(EXPLORATORY_DIR, f"dist_{var}.png"), dpi=150) # Guardar figura
                plt.close() # Cerrar figura
    
    num_vars = df.select_dtypes(include=[np.number]).columns.tolist() # Variables num√©ricas
    if num_vars:
        stats = df[num_vars].describe().T # Estad√≠sticas descriptivas
        stats.to_excel(os.path.join(TABLES_DIR, "estadisticas_numericas.xlsx")) # Guardar estad√≠sticas en Excel
        
        for var in num_vars[:10]:# Limitar a las primeras 10 variables num√©ricas
            if df[var].notna().sum() > 0:# Solo si hay datos no nulos
                plt.figure(figsize=(8, 5))# Histograma
                df[var].hist(bins=30, color='coral', edgecolor='black')# Histograma
                plt.title(f"Distribuci√≥n de {var}")# T√≠tulo del gr√°fico
                plt.xlabel(var)# Etiqueta del eje X
                plt.ylabel("Frecuencia")# Etiqueta del eje Y
                plt.tight_layout() # Ajuste del dise√±o
                plt.savefig(os.path.join(EXPLORATORY_DIR, f"hist_{var}.png"), dpi=150)# Guardar figura
                plt.close() # Cerrar figura
    
    print(f" An√°lisis exploratorio: {len(cat_vars)} categ√≥ricas, {len(num_vars)} num√©ricas") # Resumen del an√°lisis

# ============================================================
# 8. AN√ÅLISIS ESPACIO-TEMPORAL (robusto y mantiene 99999)
# ============================================================

def analyze_spatiotemporal(df):
    """An√°lisis espacial y temporal seg√∫n objetivos del proyecto"""
    print("\n Iniciando an√°lisis espacio-temporal...")

    results = {}

    # Buscar columna de localidad
    loc_col = next((c for c in df.columns if 'LOCALIDAD' in c), None)
    if not loc_col:
        print("  ‚ö†Ô∏è No se encontr√≥ columna de localidad.")
        return results

    # Verificar que exista columna de a√±o
    if 'A√ëO' not in df.columns:
        print("  ‚ö†Ô∏è No se encontr√≥ columna de a√±o para an√°lisis temporal.")
        return results

    # Asegurar que el a√±o sea num√©rico
    df['A√ëO'] = pd.to_numeric(df['A√ëO'], errors='coerce')

    # Eliminar filas sin a√±o v√°lido (manteniendo los 99999)
    df_temp = df[df['A√ëO'].notna()].copy()
    if df_temp.empty:
        print("  ‚ö†Ô∏è No hay registros con a√±o v√°lido para analizar.")
        return results

    # Agrupar intervenciones por localidad y a√±o
    distribucion = (
        df_temp.groupby([loc_col, 'A√ëO'])
        .size()
        .reset_index(name='Intervenciones')
    )

    if distribucion.empty:
        print("  ‚ö†Ô∏è No se encontraron combinaciones v√°lidas de localidad y a√±o.")
        return results

    distribucion.to_csv(os.path.join(TABLES_DIR, "distribucion_localidad_a√±o.csv"), index=False)

    # Tabla pivote (localidad √ó a√±o)
    pivot = distribucion.pivot(index=loc_col, columns='A√ëO', values='Intervenciones').fillna(0)
    pivot.to_excel(os.path.join(TABLES_DIR, "matriz_localidad_a√±o.xlsx"))

    # Calcular totales y promedios
    pivot['Total'] = pivot.sum(axis=1)
    a√±os_disponibles = [c for c in pivot.columns if isinstance(c, (int, float, np.integer, np.floating))]
    pivot['Promedio_anual'] = pivot['Total'] / max(1, len(a√±os_disponibles))

    # Calcular cambios solo si hay al menos 2 a√±os
    if len(a√±os_disponibles) >= 2:
        a√±o_ini = int(min(a√±os_disponibles))
        a√±o_fin = int(max(a√±os_disponibles))

        pivot['Cambio_absoluto'] = pivot[a√±o_fin] - pivot[a√±o_ini]
        pivot['Cambio_porcentual'] = ((pivot[a√±o_fin] - pivot[a√±o_ini]) /
                                      (pivot[a√±o_ini] + 1) * 100).round(2)

        pivot['Tendencia'] = pivot['Cambio_porcentual'].apply(
            lambda x: 'Aumento fuerte' if x > 20 else
                      ('Aumento moderado' if x > 5 else
                       ('Estable' if x > -5 else
                        ('Disminuci√≥n moderada' if x > -20 else 'Disminuci√≥n fuerte')))
        )

        pivot.to_excel(os.path.join(TABLES_DIR, "tendencias_por_localidad.xlsx"))

        # Identificar zonas de alerta
        alertas = pivot[abs(pivot['Cambio_porcentual']) > 20].sort_values('Cambio_porcentual', ascending=False)
        alertas.to_csv(ALERT_ZONES_FILE)
        print(f"    {len(alertas)} zonas de alerta identificadas")
        results['zonas_alerta'] = alertas
    else:
        print("  ‚ö†Ô∏è No hay suficientes a√±os distintos para calcular tendencias.")

    # Gr√°fico de evoluci√≥n temporal (solo si hay datos)
    top_localidades = pivot.nlargest(10, 'Total').index
    if a√±os_disponibles and len(top_localidades) > 0:
        plt.figure(figsize=(14, 8))
        for loc in top_localidades:
            valores = pivot.loc[loc, a√±os_disponibles]
            plt.plot(a√±os_disponibles, valores, marker='o', label=loc, linewidth=2)

        plt.title("Evoluci√≥n temporal de intervenciones - Top 10 localidades", fontsize=14, fontweight='bold')
        plt.xlabel("A√±o", fontsize=12)
        plt.ylabel("N√∫mero de intervenciones", fontsize=12)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(TEMPORAL_DIR, "evolucion_temporal_top10.png"), dpi=150, bbox_inches='tight')
        plt.close()

    # Gr√°fico de mapa de calor
    if a√±os_disponibles and not pivot[a√±os_disponibles].empty:
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            pivot[a√±os_disponibles].head(15),
            annot=True, fmt='.0f', cmap='YlOrRd',
            cbar_kws={'label': 'Intervenciones'}
        )
        plt.title("Mapa de calor: Intervenciones por localidad y a√±o", fontsize=14, fontweight='bold')
        plt.ylabel("Localidad", fontsize=12)
        plt.xlabel("A√±o", fontsize=12)
        plt.tight_layout()
        plt.savefig(os.path.join(SPATIAL_DIR, "heatmap_localidad_a√±o.png"), dpi=150)
        plt.close()
    else:
        print("  ‚ö†Ô∏è No se gener√≥ el mapa de calor (sin datos v√°lidos para graficar).")

    print(" An√°lisis espacio-temporal completado")
    results['distribucion'] = distribucion
    results['pivot'] = pivot

    return results


# ============================================================
# 9. AN√ÅLISIS DE R√âGIMEN SUBSIDIADO (NUEVO)
# ============================================================

def analyze_health_regime(df):
    """An√°lisis del r√©gimen de salud (Objetivo espec√≠fico 3)"""
    print("\n Analizando r√©gimen de afiliaci√≥n en salud...")# Analizando r√©gimen de afiliaci√≥n en salud
    
    afiliacion_col = None
    for col in df.columns:
        if 'AFILIACI√ìN' in col or 'AFILIACION' in col or 'SGSSS' in col or 'REGIMEN' in col: # Buscar columna relacionada
            afiliacion_col = col
            break # Detener b√∫squeda al encontrar la primera coincidencia
    
    if not afiliacion_col:
        print(" No se encontr√≥ columna de r√©gimen de salud") # Advertencia si no se encuentra
        return {} # Retornar resultados vac√≠os
    
    regimen_dist = df[afiliacion_col].value_counts() # Distribuci√≥n del r√©gimen de salud
    regimen_dist.to_csv(os.path.join(TABLES_DIR, "distribucion_regimen_salud.csv")) # Guardar distribuci√≥n en CSV
    
    plt.figure(figsize=(10, 6)) # Gr√°fico de barras del r√©gimen de salud
    regimen_dist.plot(kind='bar', color=['#2ecc71', '#e74c3c', '#3498db', '#95a5a6'])# Gr√°fico de barras del r√©gimen de salud
    plt.title("Distribuci√≥n por r√©gimen de afiliaci√≥n en salud", fontsize=14, fontweight='bold') # T√≠tulo del gr√°fico
    plt.xlabel("Tipo de r√©gimen", fontsize=12)
    plt.ylabel("N√∫mero de intervenciones", fontsize=12)
    plt.xticks(rotation=45, ha='right')# Rotar etiquetas del eje X
    plt.tight_layout()# Ajuste del dise√±o
    plt.savefig(os.path.join(EXPLORATORY_DIR, "distribucion_regimen.png"), dpi=150) # Guardar figura
    plt.close() # Cerrar figura
    
    loc_col = next((c for c in df.columns if 'LOCALIDAD' in c), None) # Columna de localidad
    
    if loc_col:
        cruce = pd.crosstab(df[loc_col], df[afiliacion_col], normalize='index') * 100 # Porcentaje por localidad
        cruce.to_excel(os.path.join(TABLES_DIR, "regimen_por_localidad_porcentaje.xlsx")) # Guardar cruce en Excel
        
        if 'SUBSIDIADO' in cruce.columns or any('SUBSID' in str(col) for col in cruce.columns):# Buscar columna relacionada con subsidiado
            subsid_col = next((c for c in cruce.columns if 'SUBSID' in str(c)), None) # Columna de subsidiado
            if subsid_col:
                top_subsidiado = cruce[subsid_col].sort_values(ascending=False).head(10) # Top 10 localidades con mayor % subsidiado
                top_subsidiado.to_csv(os.path.join(TABLES_DIR, "top_localidades_subsidiado.csv"))
                
                plt.figure(figsize=(10, 7))# Gr√°fico de barras horizontales para top 10 subsidiado
                top_subsidiado.plot(kind='barh', color='#e67e22')# Gr√°fico de barras horizontales para top 10 subsidiado
                plt.title("Top 10 localidades con mayor % r√©gimen subsidiado", fontsize=14, fontweight='bold')# T√≠tulo del gr√°fico
                plt.xlabel("Porcentaje (%)", fontsize=12)
                plt.ylabel("Localidad", fontsize=12)
                plt.tight_layout()# Ajuste del dise√±o
                plt.savefig(os.path.join(SPATIAL_DIR, "top_subsidiado_localidades.png"), dpi=150)# Guardar figura
                plt.close()# Cerrar figura
        
        plt.figure(figsize=(14, 8))# Gr√°fico de barras apiladas por localidad
        cruce.head(15).plot(kind='barh', stacked=True, colormap='Set3', figsize=(14, 8))# Gr√°fico de barras apiladas por localidad
        plt.title("Composici√≥n de r√©gimen de salud por localidad (Top 15)", fontsize=14, fontweight='bold')# T√≠tulo del gr√°fico
        plt.xlabel("Porcentaje (%)", fontsize=12)
        plt.ylabel("Localidad", fontsize=12)
        plt.legend(title='R√©gimen', bbox_to_anchor=(1.05, 1), loc='upper left') # Leyenda fuera del gr√°fico
        plt.tight_layout()
        plt.savefig(os.path.join(SPATIAL_DIR, "regimen_por_localidad_stacked.png"), dpi=150, bbox_inches='tight')# Guardar figura
        plt.close()# Cerrar figura
    
    print(" An√°lisis de r√©gimen de salud completado")# Confirmar finalizaci√≥n
    return {'distribucion': regimen_dist}# Retornar resultados


# ============================================================
# 10. CRUCES DE VARIABLES CLAVE
# ============================================================

def cross_analysis(df):
    """An√°lisis de cruces entre variables importantes"""
    print("\nüîó Generando cruces de variables...")
    
    loc_col = next((c for c in df.columns if 'LOCALIDAD' in c), None)# Columna de localidad
    tipo_col = next((c for c in df.columns if 'TIPO' in c and 'INTERVENCION' in c), None)# Columna de tipo de intervenci√≥n
    motivo_col = next((c for c in df.columns if 'MOTIVO' in c), None)# Columna de motivo
    servicio_col = next((c for c in df.columns if 'SERVICIO' in c), None)# Columna de servicio
    
    cruces_generados = 0 # Contador de cruces generados
    
    if loc_col and tipo_col:# Cruce localidad √ó tipo de intervenci√≥n
        cruce = pd.crosstab(df[loc_col], df[tipo_col])# Cruce localidad √ó tipo de intervenci√≥n
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_localidad_tipo_intervencion.xlsx"))# Guardar cruce en Excel
        cruces_generados += 1# Incrementar contador
        
        plt.figure(figsize=(12, 8))# Mapa de calor del cruce
        sns.heatmap(cruce.head(15), annot=True, fmt='d', cmap='Blues')# Mapa de calor del cruce
        plt.title("Cruce: Localidad √ó Tipo de intervenci√≥n", fontsize=14, fontweight='bold')# T√≠tulo del gr√°fico
        plt.tight_layout()# Ajuste del dise√±o
        plt.savefig(os.path.join(EXPLORATORY_DIR, "cruce_localidad_tipo.png"), dpi=150)# Guardar figura
        plt.close()
    
    if motivo_col and servicio_col:
        cruce = pd.crosstab(df[motivo_col], df[servicio_col])# Cruce motivo √ó servicio
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_motivo_servicio.xlsx"))# Guardar cruce en Excel
        cruces_generados += 1# Incrementar contador
    
    if 'A√ëO' in df.columns and tipo_col:# Cruce a√±o √ó tipo de intervenci√≥n
        cruce = pd.crosstab(df['A√ëO'], df[tipo_col])# Cruce a√±o √ó tipo de intervenci√≥n
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_a√±o_tipo_intervencion.xlsx"))# Guardar cruce en Excel
        cruces_generados += 1
    
    print(f" {cruces_generados} cruces de variables generados")# Confirmar n√∫mero de cruces generados


# ============================================================
# 11. VISUALIZACIONES GENERALES
# ============================================================

def plot_missing(df):
    """Gr√°fico de valores faltantes"""# Gr√°fico de valores faltantes
    missing = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(20)# Top 20 variables con m√°s nulos
    
    if len(missing) > 0:# Solo si hay variables con nulos
        plt.figure(figsize=(10, 8))# Gr√°fico de barras horizontales
        sns.barplot(x=missing.values, y=missing.index, palette='Reds_r')# Gr√°fico de barras horizontales
        plt.title("Porcentaje de valores faltantes (Top 20)", fontsize=14, fontweight='bold')# T√≠tulo del gr√°fico
        plt.xlabel("Porcentaje (%)", fontsize=12)# Etiqueta del eje X
        plt.ylabel("Variable", fontsize=12)# Etiqueta del eje Y
        plt.tight_layout()
        plt.savefig(os.path.join(EXPLORATORY_DIR, "missing_values.png"), dpi=150)# Guardar figura
        plt.close()# Cerrar figura


def plot_correlation(df):
    """Matriz de correlaci√≥n de variables num√©ricas"""
    numeric_df = df.select_dtypes(include=[np.number])# DataFrame solo con variables num√©ricas
    
    if numeric_df.shape[1] > 1:# Solo si hay m√°s de una variable num√©rica
        corr = numeric_df.corr()# Matriz de correlaci√≥n
        
        plt.figure(figsize=(12, 10))# Gr√°fico de mapa de calor
        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, 
                    square=True, linewidths=1, cbar_kws={"shrink": 0.8})# Gr√°fico de mapa de calor
        plt.title("Matriz de correlaci√≥n - Variables num√©ricas", fontsize=14, fontweight='bold')# T√≠tulo del gr√°fico
        plt.tight_layout()# Ajuste del dise√±o
        plt.savefig(os.path.join(EXPLORATORY_DIR, "correlation_matrix.png"), dpi=150)# Guardar figura
        plt.close()


# ============================================================
# 12. DOCUMENTACI√ìN MARKDOWN COMPLETA
# ============================================================

def generate_summary_md(df, quality, dic, temporal_info, spatial_results):
    """Genera reporte completo en Markdown"""
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:# Abrir archivo Markdown para escribir
        f.write("# üìä Data Understanding - Proyecto NNA Bogot√° (2021-2025)\n\n")
        f.write(f"**Fecha de generaci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")# Fecha de generaci√≥n
        f.write("---\n\n")# Separador
        
        f.write("## 2.1. Collect Initial Data\n\n")# Secci√≥n de recopilaci√≥n de datos
        f.write("La base de datos contiene registros de intervenciones con ni√±os, ni√±as y adolescentes (NNA) ")# Descripci√≥n del dataset
        f.write("en las localidades de Bogot√° durante el per√≠odo 2021-2025.\n\n")# Descripci√≥n del dataset
        f.write(f"- **Archivo origen:** `{os.path.basename(file_path)}`\n")# Archivo origen
        f.write(f"- **Filas:** {quality['total_filas']:,}\n")# Filas
        f.write(f"- **Columnas:** {quality['total_columnas']}\n")# Columnas
        f.write(f"- **Tama√±o:** {quality['memoria_mb']} MB\n\n")# Tama√±o
        
        if temporal_info.get('years_available'):# A√±os disponibles
            f.write(f"- **A√±os disponibles:** {', '.join(map(str, temporal_info['years_available']))}\n\n")# A√±os disponibles
        
        f.write("## 2.2. Describe Data\n\n")# Secci√≥n de descripci√≥n de datos
        f.write("### Calidad de datos\n\n")# Secci√≥n de calidad de datos
        f.write(f"- Filas duplicadas: **{quality['filas_duplicadas']}** ({quality['porcentaje_duplicados']}%)\n")# Filas duplicadas
        f.write(f"- Promedio de valores nulos: **{quality['promedio_nulos']}%**\n")# Promedio de nulos
        f.write(f"- Columnas constantes: **{quality['columnas_constantes']}**\n")# Columnas constantes
        f.write(f"- Columnas casi vac√≠as (>90% nulos): **{quality['columnas_casi_vacias']}**\n\n")# Columnas casi vac√≠as
        
        f.write("### Clasificaci√≥n de variables\n\n")# Secci√≥n de clasificaci√≥n de variables
        clasificacion = dic['Clasificaci√≥n'].value_counts()# Conteo por tipo
        for tipo, count in clasificacion.items():# Conteo por tipo
            f.write(f"- {tipo}: **{count}** variables\n")# Conteo por tipo
        f.write("\n")# Nueva l√≠nea
        
        if quality.get('columnas_problem√°ticas'):# Columnas problem√°ticas
            f.write("###  Columnas problem√°ticas detectadas\n\n")# Columnas problem√°ticas detectadas
            for item in quality['columnas_problem√°ticas'][:10]:# Limitar a las primeras 10 problem√°ticas
                f.write(f"- `{item['columna']}`: {', '.join(item['problemas'])}\n")# Listar problem√°ticas
            f.write("\n")
        
        f.write("## 2.3. Explore Data\n\n")
        f.write("### An√°lisis espacio-temporal\n\n")
        
        if spatial_results.get('zonas_alerta') is not None and len(spatial_results['zonas_alerta']) > 0:
            f.write(f"**Zonas de alerta identificadas:** {len(spatial_results['zonas_alerta'])}\n\n")
            f.write("Localidades con cambios significativos (>20%):\n\n")# Localidades con cambios significativos (>20%):
            f.write("| Localidad | Cambio % |\n")# Tabla de cambios
            f.write("|-----------|----------|\n")# Encabezado de la tabla
            for idx, row in spatial_results['zonas_alerta'].head(10).iterrows():# Limitar a las primeras 10
                f.write(f"| {idx} | {row['Cambio_porcentual']:.1f}% |\n")
            f.write("\n")
        
        f.write("### Distribuci√≥n geogr√°fica\n\n")
        f.write("Se generaron an√°lisis detallados de:\n")
        f.write("- Distribuci√≥n de intervenciones por localidad y a√±o\n")
        f.write("- Evoluci√≥n temporal de las principales localidades\n")# Evoluci√≥n temporal de las principales localidades
        f.write("- Identificaci√≥n de tendencias (aumento/disminuci√≥n)\n")# Identificaci√≥n de tendencias (aumento/disminuci√≥n)
        f.write("- Relaci√≥n con r√©gimen de afiliaci√≥n en salud\n\n")# Relaci√≥n con r√©gimen de afiliaci√≥n en salud
        
        f.write("## 2.4. Verify Data Quality\n\n")
        f.write("### Verificaciones realizadas\n\n")#
        f.write(" **Completitud:** An√°lisis de valores faltantes por variable\n\n")
        f.write(" **Consistencia:** Detecci√≥n de duplicados y valores constantes\n\n")
        f.write(" **Validez:** Identificaci√≥n de columnas con alta cardinalidad\n\n")
        f.write(" **Unicidad:** Verificaci√≥n de identificadores √∫nicos\n\n")
        
        f.write("##  Archivos generados\n\n")
        f.write("### Tablas\n")
        f.write("- `diccionario_datos.xlsx` - Descripci√≥n completa de variables\n")
        f.write("- `distribucion_localidad_a√±o.csv` - Datos espacio-temporales\n")
        f.write("- `tendencias_por_localidad.xlsx` - An√°lisis de cambios\n")
        f.write("- `zonas_alerta.csv` - Localidades con cambios significativos\n")
        f.write("- `regimen_por_localidad_porcentaje.xlsx` - Distribuci√≥n de afiliaci√≥n\n")
        f.write("- `quality_report.json` - Reporte completo de calidad\n\n")
        
        f.write("### Figuras\n")
        f.write("- **Temporales:** Evoluci√≥n de intervenciones por a√±o\n")
        f.write("- **Espaciales:** Mapas de calor y distribuciones por localidad\n")
        f.write("- **Exploratorias:** Distribuciones, correlaciones y valores faltantes\n\n")
        
        f.write("---\n\n")
        f.write(f"**Ruta de reportes:** `{REPORTS_DIR}`\n")
    
    print(" Reporte Markdown generado correctamente")


# ============================================================
# 13. FLUJO PRINCIPAL
# ============================================================

def main():
    """Flujo principal de Data Understanding"""
    print("="*70)
    print(" DATA UNDERSTANDING - PROYECTO NNA BOGOT√Å (2021-2025)")
    print("="*70)
    print()
    
    # 1. Carga de datos
    print(" PASO 1: Carga de datos")
    print("-"*70)
    df = load_data(file_path)# Cargar datos
    print()
    
    # 2. Limpieza y normalizaci√≥n
    print(" PASO 2: Limpieza y normalizaci√≥n") 
    print("-"*70) # Limpieza y normalizaci√≥n
    df = clean_columns(df)
    df = anonymize(df)
    print()
    
    # 3. Detecci√≥n temporal
    print(" PASO 3: Detecci√≥n de variables temporales")
    print("-"*70)
    df, temporal_info = detect_temporal_columns(df)
    print()
    
    # 4. Diccionario de datos
    print(" PASO 4: Generaci√≥n de diccionario de datos")
    print("-"*70)
    dic = generate_dictionary(df)
    print()
    
    # 5. Verificaci√≥n de calidad
    print("  PASO 5: Verificaci√≥n de calidad")# Verificaci√≥n de calidad
    print("-"*70)
    quality = verify_quality(df)
    print()
    
    # 6. An√°lisis exploratorio
    print(" PASO 6: An√°lisis exploratorio")
    print("-"*70)
    exploratory_analysis(df, dic)
    print()
    
    # 7. An√°lisis espacio-temporal
    print("  PASO 7: An√°lisis espacio-temporal") 
    print("-"*70)
    spatial_results = analyze_spatiotemporal(df)
    print()
    
    # 8. An√°lisis de r√©gimen de salud
    print(" PASO 8: An√°lisis de r√©gimen de salud")
    print("-"*70)
    analyze_health_regime(df)
    print()
    
    # 9. Cruces de variables
    print("PASO 9: Cruces de variables clave")#
    print("-"*70)#
    cross_analysis(df)
    print()
    
    # 10. Visualizaciones generales
    print("üìä PASO 10: Visualizaciones generales")
    print("-"*70)#
    plot_missing(df)
    plot_correlation(df)
    print(" Visualizaciones generales completadas")
    print()
    
    # 11. Documentaci√≥n final
    print(" PASO 11: Generaci√≥n de documentaci√≥n")
    print("-"*70)
    generate_summary_md(df, quality, dic, temporal_info, spatial_results)
    print()
    
    # Resumen final
    print("="*70)
    print(" DATA UNDERSTANDING COMPLETADO CON √âXITO")
    print("="*70)
    print()
    print(f" Resumen:")
    print(f"   ‚Ä¢ Filas procesadas: {df.shape[0]:,}")
    print(f"   ‚Ä¢ Columnas analizadas: {df.shape[1]}")
    print(f"   ‚Ä¢ Duplicados detectados: {quality['filas_duplicadas']}")
    print(f"   ‚Ä¢ Calidad promedio: {100 - quality['promedio_nulos']:.1f}%")
    print()
    print(f" Archivos generados en:")
    print(f"   ‚Ä¢ Reportes: {REPORTS_DIR}")
    print(f"   ‚Ä¢ Tablas: {TABLES_DIR}")# Tablas
    print(f"   ‚Ä¢ Figuras: {FIGURES_DIR}")# Figuras
    print()
    print(f" Resumen completo disponible en:")
    print(f"   {SUMMARY_FILE}") 
    print()
    print("="*70)


if __name__ == "__main__": # Punto de entrada del script
    try:
        main()# Ejecutar flujo principal
    except Exception as e:
        print(f"\n ERROR: {str(e)}")# Capturar cualquier error inesperado
        import traceback # Importar m√≥dulo traceback
        traceback.print_exc() # Imprimir traceback del error

# ============================================================
# FIN DEL SCRIPT
# ============================================================