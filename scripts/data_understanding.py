# ============================================================
# DATA UNDERSTANDING - Proyecto NNA Bogot√° (2021‚Äì2025)
# Versi√≥n extendida con an√°lisis espacio-temporal
# ============================================================

import os
import pandas as pd
import numpy as np
import hashlib
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv
from datetime import datetime
import warnings
import json

warnings.filterwarnings('ignore')
sns.set_style("whitegrid")

# ============================================================
# 1. CONFIGURACI√ìN DE RUTAS
# ============================================================

load_dotenv()

BASE_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_FILE = os.getenv("DATA_FILE")

if not DATA_FILE:
    raise ValueError("‚ùå No se encontr√≥ la variable DATA_FILE en el archivo .env")

file_path = os.path.join(BASE_DIR, DATA_FILE)

if not os.path.exists(file_path):
    raise FileNotFoundError(f"‚ùå No se encontr√≥ el archivo: {file_path}")

# Estructura de carpetas
REPORTS_DIR = os.path.join(BASE_DIR, "reports")
FIGURES_DIR = os.path.join(REPORTS_DIR, "figures")
TABLES_DIR = os.path.join(REPORTS_DIR, "tables")
TEMPORAL_DIR = os.path.join(FIGURES_DIR, "temporal")
SPATIAL_DIR = os.path.join(FIGURES_DIR, "spatial")
EXPLORATORY_DIR = os.path.join(FIGURES_DIR, "exploratory")
SUMMARY_FILE = os.path.join(REPORTS_DIR, "data_summary.md")
ALERT_ZONES_FILE = os.path.join(TABLES_DIR, "zonas_alerta.csv")

for path in [REPORTS_DIR, FIGURES_DIR, TABLES_DIR, TEMPORAL_DIR, SPATIAL_DIR, EXPLORATORY_DIR]:
    os.makedirs(path, exist_ok=True)

print(f"üìÅ Estructura de carpetas creada en: {REPORTS_DIR}")


# ============================================================
# 2. CARGA DE DATOS CON DETECCI√ìN AUTOM√ÅTICA
# ============================================================

def load_data(file_path):
    """Carga datos desde Excel o CSV con detecci√≥n autom√°tica"""
    ext = os.path.splitext(file_path)[-1].lower()
    
    print(f"üìÇ Cargando archivo: {os.path.basename(file_path)}")
    
    if ext in ['.xlsx', '.xls']:
        xls = pd.ExcelFile(file_path)
        print(f"   Hojas disponibles: {xls.sheet_names}")
        sheet = 'BD' if 'BD' in xls.sheet_names else xls.sheet_names[-1]
        df = pd.read_excel(file_path, sheet_name=sheet)
        print(f"   ‚úì Cargada hoja: {sheet}")
    elif ext == '.csv':
        df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8')
        print(f"   ‚úì CSV cargado con detecci√≥n autom√°tica de separador")
    else:
        raise ValueError("‚ùå Formato no compatible. Usa .csv, .xlsx o .xls")
    
    print(f"‚úÖ Datos cargados: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
    return df


# ============================================================
# 3. LIMPIEZA Y NORMALIZACI√ìN
# ============================================================

def clean_columns(df):
    """Limpia y normaliza nombres de columnas"""
    original_cols = df.columns.tolist()
    
    df.columns = (
        df.columns
        .str.strip()
        .str.replace(' ', '_')
        .str.replace(r'[^\w]', '', regex=True)
        .str.upper()
    )
    
    col_mapping = dict(zip(original_cols, df.columns))
    with open(os.path.join(TABLES_DIR, "column_mapping.json"), "w", encoding="utf-8") as f:
        json.dump(col_mapping, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Columnas normalizadas: {len(df.columns)} variables")
    return df


def anonymize(df):
    """Anonimiza informaci√≥n sensible sin afectar variables geogr√°ficas."""
    sensitive_cols = [
        'USUARIO', 'NOMBRE', 'APELLIDO', 'DOCUMENTO', 'ID_USUARIO',
        'DIRECCI√ìN', 'DIRECCION', 'DIRECCI√ìN_DE_LA_VIVIENDA', 'DIRECCION_VIVIENDA',
        'CORREO', 'CORREO_1', 'CORREO_2', 'EMAIL', 'MAIL',
        'TEL√âFONO', 'TELEFONO', 'TEL√âFONO_1', 'TEL√âFONO_2', 'TELEFONO_1', 'TELEFONO_2', 'CELULAR',
        'NOMBRE_EAPB', 'NOMBRE_EAPB1', 'RESPONSABLE', 'ACUDIENTE'
    ]
    
    # Eliminar columnas sensibles si existen
    removed = [c for c in sensitive_cols if c in df.columns]
    df.drop(columns=removed, inplace=True, errors='ignore')
    
    # Anonimizar solo columnas que contengan "ID" pero sin tocar las geogr√°ficas o de referencia territorial
    id_cols = [
        c for c in df.columns 
        if 'ID' in c 
        and c not in ['ID_LOCALIDAD', 'ID_UPZ', 'LOCALIDAD', 'LOCALIDAD_FIC']
    ]
    
    # Aplicar hash a las columnas identificadas como ID
    for col in id_cols:
        if df[col].dtype == 'object' or pd.api.types.is_integer_dtype(df[col]):
            df[col] = df[col].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:16] if pd.notna(x) else x)
    
    print(f"‚úÖ Anonimizaci√≥n completa: {len(removed)} columnas eliminadas, {len(id_cols)} IDs encriptados (sin afectar LOCALIDAD ni LOCALIDAD_FIC)")
    return df



# ============================================================
# 4. DETECCI√ìN AUTOM√ÅTICA DE VARIABLES TEMPORALES
# ============================================================

def detect_temporal_columns(df):
    """Detecta y procesa columnas de fecha/a√±o"""
    temporal_info = {}
    
    date_candidates = [c for c in df.columns if any(x in c for x in ['FECHA', 'DATE', 'A√ëO', 'ANO', 'YEAR', 'MES', 'MONTH'])]
    
    for col in date_candidates:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            temporal_info[col] = 'datetime'
        elif df[col].dtype == 'object':
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                temporal_info[col] = 'datetime_converted'
            except:
                pass
    
    for col in temporal_info.keys():
        if 'A√ëO' not in df.columns and 'YEAR' not in df.columns:
            df['A√ëO'] = df[col].dt.year
            print(f"   ‚úì Columna 'A√ëO' extra√≠da de {col}")
    
    year_cols = [c for c in df.columns if c in ['A√ëO', 'ANO', 'YEAR', 'ANIO']]
    if year_cols:
        df['A√ëO'] = pd.to_numeric(df[year_cols[0]], errors='coerce')
    
    if 'A√ëO' in df.columns:
        years = df['A√ëO'].dropna().unique()
        print(f"‚úÖ A√±os detectados: {sorted([int(y) for y in years if not np.isnan(y)])}")
        temporal_info['years_available'] = sorted([int(y) for y in years if not np.isnan(y)])
    else:
        print("‚ö†Ô∏è  No se detect√≥ columna de a√±o")
    
    return df, temporal_info


# ============================================================
# 5. DICCIONARIO DE DATOS AMPLIADO
# ============================================================

def generate_dictionary(df):
    """Genera diccionario completo de datos"""
    dic = pd.DataFrame({
        "Variable": df.columns,
        "Tipo_dato": df.dtypes.values,
        "Valores_nulos": df.isnull().sum().values,
        "Porcentaje_nulos": (df.isnull().sum() / len(df) * 100).round(2).values,
        "Valores_√∫nicos": df.nunique().values,
        "Cardinalidad": df.nunique().values / len(df) * 100,
        "Primer_valor": [df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else None for col in df.columns]
    })
    
    dic['Clasificaci√≥n'] = dic.apply(lambda row: clasificar_variable(row, len(df)), axis=1)
    
    dic.to_excel(os.path.join(TABLES_DIR, "diccionario_datos.xlsx"), index=False)
    dic.to_csv(os.path.join(TABLES_DIR, "diccionario_datos.csv"), index=False)
    
    print(f"‚úÖ Diccionario de datos generado: {len(dic)} variables documentadas")
    return dic


def clasificar_variable(row, n_rows):
    """Clasifica tipo de variable seg√∫n sus caracter√≠sticas"""
    if row['Porcentaje_nulos'] > 90:
        return 'Casi_vac√≠a'
    elif row['Valores_√∫nicos'] == 1:
        return 'Constante'
    elif row['Valores_√∫nicos'] == n_rows:
        return 'Identificador'
    elif row['Valores_√∫nicos'] <= 10:
        return 'Categ√≥rica_baja'
    elif row['Valores_√∫nicos'] <= 50:
        return 'Categ√≥rica_media'
    elif row['Cardinalidad'] > 90:
        return 'Alta_cardinalidad'
    elif 'int' in str(row['Tipo_dato']) or 'float' in str(row['Tipo_dato']):
        return 'Num√©rica'
    else:
        return 'Categ√≥rica_alta'


# ============================================================
# 6. VERIFICACI√ìN DE CALIDAD DE DATOS (Versi√≥n corregida)
# ============================================================

def verify_quality(df):
    """An√°lisis exhaustivo de calidad de datos"""
    quality = {
        "total_filas": int(df.shape[0]),
        "total_columnas": int(df.shape[1]),
        "filas_duplicadas": int(df.duplicated().sum()),
        "porcentaje_duplicados": float((df.duplicated().sum() / len(df) * 100).round(2)),
        "promedio_nulos": float((df.isnull().mean().mean() * 100).round(2)),
        "columnas_constantes": int(len([c for c in df.columns if df[c].nunique() == 1])),
        "columnas_casi_vacias": int(len([c for c in df.columns if df[c].isnull().sum() / len(df) > 0.9])),
        "memoria_mb": float((df.memory_usage(deep=True).sum() / 1024**2).round(2))
    }

    # Identificar columnas problem√°ticas
    problematic = []
    for col in df.columns:
        issues = []
        if df[col].nunique() == 1:
            issues.append("constante")
        if df[col].isnull().sum() / len(df) > 0.9:
            issues.append("casi_vac√≠a")
        if df[col].nunique() > len(df) * 0.95 and df[col].dtype == 'object':
            issues.append("alta_cardinalidad")
        if issues:
            problematic.append({"columna": col, "problemas": issues})

    quality["columnas_problem√°ticas"] = problematic

    # ‚úÖ Convertir todos los valores a tipos compatibles con JSON
    quality_serializable = {
        k: (v.tolist() if hasattr(v, "tolist") else v)
        for k, v in quality.items()
    }

    # Guardar en JSON y CSV
    with open(os.path.join(TABLES_DIR, "quality_report.json"), "w", encoding="utf-8") as f:
        json.dump(quality_serializable, f, indent=2, ensure_ascii=False)

    pd.Series({k: v for k, v in quality_serializable.items() if not isinstance(v, list)}).to_csv(
        os.path.join(TABLES_DIR, "quality_summary.csv")
    )

    print(f"‚úÖ Calidad verificada: {quality['filas_duplicadas']} duplicados, {quality['columnas_constantes']} columnas constantes")
    return quality

# ============================================================
# 7. AN√ÅLISIS EXPLORATORIO AMPLIADO
# ============================================================

def exploratory_analysis(df, dic):
    """An√°lisis exploratorio completo de todas las variables"""
    print("\nüîç Iniciando an√°lisis exploratorio ampliado...")
    
    cat_vars = dic[dic['Clasificaci√≥n'].str.contains('Categ√≥rica', na=False)]['Variable'].tolist()
    
    for var in cat_vars[:15]:
        if var in df.columns:
            freq = df[var].value_counts().head(20)
            freq.to_csv(os.path.join(TABLES_DIR, f"frecuencia_{var}.csv"))
            
            if len(freq) > 0:
                plt.figure(figsize=(10, 6))
                freq.plot(kind='barh', color='steelblue')
                plt.title(f"Distribuci√≥n de {var}")
                plt.xlabel("Frecuencia")
                plt.tight_layout()
                plt.savefig(os.path.join(EXPLORATORY_DIR, f"dist_{var}.png"), dpi=150)
                plt.close()
    
    num_vars = df.select_dtypes(include=[np.number]).columns.tolist()
    if num_vars:
        stats = df[num_vars].describe().T
        stats.to_excel(os.path.join(TABLES_DIR, "estadisticas_numericas.xlsx"))
        
        for var in num_vars[:10]:
            if df[var].notna().sum() > 0:
                plt.figure(figsize=(8, 5))
                df[var].hist(bins=30, color='coral', edgecolor='black')
                plt.title(f"Distribuci√≥n de {var}")
                plt.xlabel(var)
                plt.ylabel("Frecuencia")
                plt.tight_layout()
                plt.savefig(os.path.join(EXPLORATORY_DIR, f"hist_{var}.png"), dpi=150)
                plt.close()
    
    print(f"‚úÖ An√°lisis exploratorio: {len(cat_vars)} categ√≥ricas, {len(num_vars)} num√©ricas")


# ============================================================
# 8. AN√ÅLISIS ESPACIO-TEMPORAL (NUEVO)
# ============================================================

def analyze_spatiotemporal(df):
    """An√°lisis espacial y temporal seg√∫n objetivos del proyecto"""
    print("\nüìä Iniciando an√°lisis espacio-temporal...")
    
    results = {}
    
    loc_col = None
    for col in df.columns:
        if 'LOCALIDAD' in col:
            loc_col = col
            break
    
    if not loc_col or 'A√ëO' not in df.columns:
        print("‚ö†Ô∏è  No se encontraron columnas de localidad o a√±o necesarias")
        return results
    
    distribucion = df.groupby([loc_col, 'A√ëO']).size().reset_index(name='Intervenciones')
    distribucion.to_csv(os.path.join(TABLES_DIR, "distribucion_localidad_a√±o.csv"), index=False)
    
    pivot = distribucion.pivot(index=loc_col, columns='A√ëO', values='Intervenciones').fillna(0)
    pivot.to_excel(os.path.join(TABLES_DIR, "matriz_localidad_a√±o.xlsx"))
    
    pivot['Total'] = pivot.sum(axis=1)
    pivot['Promedio_anual'] = pivot['Total'] / len(pivot.columns)
    
    if len(pivot.columns) >= 3:
        a√±os = [col for col in pivot.columns if isinstance(col, (int, np.integer))]
        if len(a√±os) >= 2:
            pivot['Cambio_absoluto'] = pivot[max(a√±os)] - pivot[min(a√±os)]
            pivot['Cambio_porcentual'] = ((pivot[max(a√±os)] - pivot[min(a√±os)]) / (pivot[min(a√±os)] + 1) * 100).round(2)
            pivot['Tendencia'] = pivot['Cambio_porcentual'].apply(
                lambda x: 'Aumento fuerte' if x > 20 else ('Aumento moderado' if x > 5 else ('Estable' if x > -5 else ('Disminuci√≥n moderada' if x > -20 else 'Disminuci√≥n fuerte')))
            )
    
    pivot.to_excel(os.path.join(TABLES_DIR, "tendencias_por_localidad.xlsx"))
    
    if 'Cambio_porcentual' in pivot.columns:
        alertas = pivot[abs(pivot['Cambio_porcentual']) > 20].sort_values('Cambio_porcentual', ascending=False)
        alertas.to_csv(ALERT_ZONES_FILE)
        print(f"   ‚ö†Ô∏è  {len(alertas)} zonas de alerta identificadas")
        results['zonas_alerta'] = alertas
    
    top_localidades = pivot.nlargest(10, 'Total').index
    
    plt.figure(figsize=(14, 8))
    a√±os_disponibles = [col for col in pivot.columns if isinstance(col, (int, np.integer))]
    
    for loc in top_localidades:
        valores = pivot.loc[loc, a√±os_disponibles]
        plt.plot(a√±os_disponibles, valores, marker='o', label=loc, linewidth=2)
    
    plt.title("Evoluci√≥n temporal de intervenciones - Top 10 localidades", fontsize=14, fontweight='bold')
    plt.xlabel("A√±o", fontsize=12)
    plt.ylabel("N√∫mero de intervenciones", fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(TEMPORAL_DIR, "evolucion_temporal_top10.png"), dpi=150, bbox_inches='tight')
    plt.close()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(pivot[a√±os_disponibles].head(15), annot=True, fmt='.0f', cmap='YlOrRd', cbar_kws={'label': 'Intervenciones'})
    plt.title("Mapa de calor: Intervenciones por localidad y a√±o", fontsize=14, fontweight='bold')
    plt.ylabel("Localidad", fontsize=12)
    plt.xlabel("A√±o", fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(SPATIAL_DIR, "heatmap_localidad_a√±o.png"), dpi=150)
    plt.close()
    
    print(f"‚úÖ An√°lisis espacio-temporal completado")
    results['distribucion'] = distribucion
    results['pivot'] = pivot
    
    return results


# ============================================================
# 9. AN√ÅLISIS DE R√âGIMEN SUBSIDIADO (NUEVO)
# ============================================================

def analyze_health_regime(df):
    """An√°lisis del r√©gimen de salud (Objetivo espec√≠fico 3)"""
    print("\nüíä Analizando r√©gimen de afiliaci√≥n en salud...")
    
    afiliacion_col = None
    for col in df.columns:
        if 'AFILIACI√ìN' in col or 'AFILIACION' in col or 'SGSSS' in col or 'REGIMEN' in col:
            afiliacion_col = col
            break
    
    if not afiliacion_col:
        print("‚ö†Ô∏è  No se encontr√≥ columna de r√©gimen de salud")
        return {}
    
    regimen_dist = df[afiliacion_col].value_counts()
    regimen_dist.to_csv(os.path.join(TABLES_DIR, "distribucion_regimen_salud.csv"))
    
    plt.figure(figsize=(10, 6))
    regimen_dist.plot(kind='bar', color=['#2ecc71', '#e74c3c', '#3498db', '#95a5a6'])
    plt.title("Distribuci√≥n por r√©gimen de afiliaci√≥n en salud", fontsize=14, fontweight='bold')
    plt.xlabel("Tipo de r√©gimen", fontsize=12)
    plt.ylabel("N√∫mero de intervenciones", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(os.path.join(EXPLORATORY_DIR, "distribucion_regimen.png"), dpi=150)
    plt.close()
    
    loc_col = next((c for c in df.columns if 'LOCALIDAD' in c), None)
    
    if loc_col:
        cruce = pd.crosstab(df[loc_col], df[afiliacion_col], normalize='index') * 100
        cruce.to_excel(os.path.join(TABLES_DIR, "regimen_por_localidad_porcentaje.xlsx"))
        
        if 'SUBSIDIADO' in cruce.columns or any('SUBSID' in str(col) for col in cruce.columns):
            subsid_col = next((c for c in cruce.columns if 'SUBSID' in str(c)), None)
            if subsid_col:
                top_subsidiado = cruce[subsid_col].sort_values(ascending=False).head(10)
                top_subsidiado.to_csv(os.path.join(TABLES_DIR, "top_localidades_subsidiado.csv"))
                
                plt.figure(figsize=(10, 7))
                top_subsidiado.plot(kind='barh', color='#e67e22')
                plt.title("Top 10 localidades con mayor % r√©gimen subsidiado", fontsize=14, fontweight='bold')
                plt.xlabel("Porcentaje (%)", fontsize=12)
                plt.ylabel("Localidad", fontsize=12)
                plt.tight_layout()
                plt.savefig(os.path.join(SPATIAL_DIR, "top_subsidiado_localidades.png"), dpi=150)
                plt.close()
        
        plt.figure(figsize=(14, 8))
        cruce.head(15).plot(kind='barh', stacked=True, colormap='Set3', figsize=(14, 8))
        plt.title("Composici√≥n de r√©gimen de salud por localidad (Top 15)", fontsize=14, fontweight='bold')
        plt.xlabel("Porcentaje (%)", fontsize=12)
        plt.ylabel("Localidad", fontsize=12)
        plt.legend(title='R√©gimen', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(SPATIAL_DIR, "regimen_por_localidad_stacked.png"), dpi=150, bbox_inches='tight')
        plt.close()
    
    print("‚úÖ An√°lisis de r√©gimen de salud completado")
    return {'distribucion': regimen_dist}


# ============================================================
# 10. CRUCES DE VARIABLES CLAVE
# ============================================================

def cross_analysis(df):
    """An√°lisis de cruces entre variables importantes"""
    print("\nüîó Generando cruces de variables...")
    
    loc_col = next((c for c in df.columns if 'LOCALIDAD' in c), None)
    tipo_col = next((c for c in df.columns if 'TIPO' in c and 'INTERVENCION' in c), None)
    motivo_col = next((c for c in df.columns if 'MOTIVO' in c), None)
    servicio_col = next((c for c in df.columns if 'SERVICIO' in c), None)
    
    cruces_generados = 0
    
    if loc_col and tipo_col:
        cruce = pd.crosstab(df[loc_col], df[tipo_col])
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_localidad_tipo_intervencion.xlsx"))
        cruces_generados += 1
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(cruce.head(15), annot=True, fmt='d', cmap='Blues')
        plt.title("Cruce: Localidad √ó Tipo de intervenci√≥n", fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(EXPLORATORY_DIR, "cruce_localidad_tipo.png"), dpi=150)
        plt.close()
    
    if motivo_col and servicio_col:
        cruce = pd.crosstab(df[motivo_col], df[servicio_col])
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_motivo_servicio.xlsx"))
        cruces_generados += 1
    
    if 'A√ëO' in df.columns and tipo_col:
        cruce = pd.crosstab(df['A√ëO'], df[tipo_col])
        cruce.to_excel(os.path.join(TABLES_DIR, "cruce_a√±o_tipo_intervencion.xlsx"))
        cruces_generados += 1
    
    print(f"‚úÖ {cruces_generados} cruces de variables generados")


# ============================================================
# 11. VISUALIZACIONES GENERALES
# ============================================================

def plot_missing(df):
    """Gr√°fico de valores faltantes"""
    missing = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(20)
    
    if len(missing) > 0:
        plt.figure(figsize=(10, 8))
        sns.barplot(x=missing.values, y=missing.index, palette='Reds_r')
        plt.title("Porcentaje de valores faltantes (Top 20)", fontsize=14, fontweight='bold')
        plt.xlabel("Porcentaje (%)", fontsize=12)
        plt.ylabel("Variable", fontsize=12)
        plt.tight_layout()
        plt.savefig(os.path.join(EXPLORATORY_DIR, "missing_values.png"), dpi=150)
        plt.close()


def plot_correlation(df):
    """Matriz de correlaci√≥n de variables num√©ricas"""
    numeric_df = df.select_dtypes(include=[np.number])
    
    if numeric_df.shape[1] > 1:
        corr = numeric_df.corr()
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, 
                    square=True, linewidths=1, cbar_kws={"shrink": 0.8})
        plt.title("Matriz de correlaci√≥n - Variables num√©ricas", fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(EXPLORATORY_DIR, "correlation_matrix.png"), dpi=150)
        plt.close()


# ============================================================
# 12. DOCUMENTACI√ìN MARKDOWN COMPLETA
# ============================================================

def generate_summary_md(df, quality, dic, temporal_info, spatial_results):
    """Genera reporte completo en Markdown"""
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("# üìä Data Understanding - Proyecto NNA Bogot√° (2021-2025)\n\n")
        f.write(f"**Fecha de generaci√≥n:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        f.write("## 2.1. Collect Initial Data\n\n")
        f.write("La base de datos contiene registros de intervenciones con ni√±os, ni√±as y adolescentes (NNA) ")
        f.write("en las localidades de Bogot√° durante el per√≠odo 2021-2025.\n\n")
        f.write(f"- **Archivo origen:** `{os.path.basename(file_path)}`\n")
        f.write(f"- **Filas:** {quality['total_filas']:,}\n")
        f.write(f"- **Columnas:** {quality['total_columnas']}\n")
        f.write(f"- **Tama√±o:** {quality['memoria_mb']} MB\n\n")
        
        if temporal_info.get('years_available'):
            f.write(f"- **A√±os disponibles:** {', '.join(map(str, temporal_info['years_available']))}\n\n")
        
        f.write("## 2.2. Describe Data\n\n")
        f.write("### Calidad de datos\n\n")
        f.write(f"- Filas duplicadas: **{quality['filas_duplicadas']}** ({quality['porcentaje_duplicados']}%)\n")
        f.write(f"- Promedio de valores nulos: **{quality['promedio_nulos']}%**\n")
        f.write(f"- Columnas constantes: **{quality['columnas_constantes']}**\n")
        f.write(f"- Columnas casi vac√≠as (>90% nulos): **{quality['columnas_casi_vacias']}**\n\n")
        
        f.write("### Clasificaci√≥n de variables\n\n")
        clasificacion = dic['Clasificaci√≥n'].value_counts()
        for tipo, count in clasificacion.items():
            f.write(f"- {tipo}: **{count}** variables\n")
        f.write("\n")
        
        if quality.get('columnas_problem√°ticas'):
            f.write("### ‚ö†Ô∏è Columnas problem√°ticas detectadas\n\n")
            for item in quality['columnas_problem√°ticas'][:10]:
                f.write(f"- `{item['columna']}`: {', '.join(item['problemas'])}\n")
            f.write("\n")
        
        f.write("## 2.3. Explore Data\n\n")
        f.write("### An√°lisis espacio-temporal\n\n")
        
        if spatial_results.get('zonas_alerta') is not None and len(spatial_results['zonas_alerta']) > 0:
            f.write(f"**Zonas de alerta identificadas:** {len(spatial_results['zonas_alerta'])}\n\n")
            f.write("Localidades con cambios significativos (>20%):\n\n")
            f.write("| Localidad | Cambio % |\n")
            f.write("|-----------|----------|\n")
            for idx, row in spatial_results['zonas_alerta'].head(10).iterrows():
                f.write(f"| {idx} | {row['Cambio_porcentual']:.1f}% |\n")
            f.write("\n")
        
        f.write("### Distribuci√≥n geogr√°fica\n\n")
        f.write("Se generaron an√°lisis detallados de:\n")
        f.write("- Distribuci√≥n de intervenciones por localidad y a√±o\n")
        f.write("- Evoluci√≥n temporal de las principales localidades\n")
        f.write("- Identificaci√≥n de tendencias (aumento/disminuci√≥n)\n")
        f.write("- Relaci√≥n con r√©gimen de afiliaci√≥n en salud\n\n")
        
        f.write("## 2.4. Verify Data Quality\n\n")
        f.write("### Verificaciones realizadas\n\n")
        f.write("‚úÖ **Completitud:** An√°lisis de valores faltantes por variable\n\n")
        f.write("‚úÖ **Consistencia:** Detecci√≥n de duplicados y valores constantes\n\n")
        f.write("‚úÖ **Validez:** Identificaci√≥n de columnas con alta cardinalidad\n\n")
        f.write("‚úÖ **Unicidad:** Verificaci√≥n de identificadores √∫nicos\n\n")
        
        f.write("## üìÅ Archivos generados\n\n")
        f.write("### Tablas\n")
        f.write("- `diccionario_datos.xlsx` - Descripci√≥n completa de variables\n")
        f.write("- `distribucion_localidad_a√±o.csv` - Datos espacio-temporales\n")
        f.write("- `tendencias_por_localidad.xlsx` - An√°lisis de cambios\n")
        f.write("- `zonas_alerta.csv` - Localidades con cambios significativos\n")
        f.write("- `regimen_por_localidad_porcentaje.xlsx` - Distribuci√≥n de afiliaci√≥n\n")
        f.write("- `quality_report.json` - Reporte completo de calidad\n\n")
        
        f.write("### Figuras\n")
        f.write("- **Temporales:** Evoluci√≥n de intervenciones por a√±o\n")
        f.write("- **Espaciales:** Mapas de calor y distribuciones por localidad\n")
        f.write("- **Exploratorias:** Distribuciones, correlaciones y valores faltantes\n\n")
        
        f.write("---\n\n")
        f.write(f"**Ruta de reportes:** `{REPORTS_DIR}`\n")
    
    print("‚úÖ Reporte Markdown generado correctamente")


# ============================================================
# 13. FLUJO PRINCIPAL
# ============================================================

def main():
    """Flujo principal de Data Understanding"""
    print("="*70)
    print("üöÄ DATA UNDERSTANDING - PROYECTO NNA BOGOT√Å (2021-2025)")
    print("="*70)
    print()
    
    # 1. Carga de datos
    print("üì• PASO 1: Carga de datos")
    print("-"*70)
    df = load_data(file_path)
    print()
    
    # 2. Limpieza y normalizaci√≥n
    print("üßπ PASO 2: Limpieza y normalizaci√≥n")
    print("-"*70)
    df = clean_columns(df)
    df = anonymize(df)
    print()
    
    # 3. Detecci√≥n temporal
    print("üìÖ PASO 3: Detecci√≥n de variables temporales")
    print("-"*70)
    df, temporal_info = detect_temporal_columns(df)
    print()
    
    # 4. Diccionario de datos
    print("üìñ PASO 4: Generaci√≥n de diccionario de datos")
    print("-"*70)
    dic = generate_dictionary(df)
    print()
    
    # 5. Verificaci√≥n de calidad
    print("‚úîÔ∏è  PASO 5: Verificaci√≥n de calidad")
    print("-"*70)
    quality = verify_quality(df)
    print()
    
    # 6. An√°lisis exploratorio
    print("üîç PASO 6: An√°lisis exploratorio")
    print("-"*70)
    exploratory_analysis(df, dic)
    print()
    
    # 7. An√°lisis espacio-temporal
    print("üó∫Ô∏è  PASO 7: An√°lisis espacio-temporal")
    print("-"*70)
    spatial_results = analyze_spatiotemporal(df)
    print()
    
    # 8. An√°lisis de r√©gimen de salud
    print("üíä PASO 8: An√°lisis de r√©gimen de salud")
    print("-"*70)
    analyze_health_regime(df)
    print()
    
    # 9. Cruces de variables
    print("üîó PASO 9: Cruces de variables clave")
    print("-"*70)
    cross_analysis(df)
    print()
    
    # 10. Visualizaciones generales
    print("üìä PASO 10: Visualizaciones generales")
    print("-"*70)
    plot_missing(df)
    plot_correlation(df)
    print("‚úÖ Visualizaciones generales completadas")
    print()
    
    # 11. Documentaci√≥n final
    print("üìù PASO 11: Generaci√≥n de documentaci√≥n")
    print("-"*70)
    generate_summary_md(df, quality, dic, temporal_info, spatial_results)
    print()
    
    # Resumen final
    print("="*70)
    print("‚úÖ DATA UNDERSTANDING COMPLETADO CON √âXITO")
    print("="*70)
    print()
    print(f"üìä Resumen:")
    print(f"   ‚Ä¢ Filas procesadas: {df.shape[0]:,}")
    print(f"   ‚Ä¢ Columnas analizadas: {df.shape[1]}")
    print(f"   ‚Ä¢ Duplicados detectados: {quality['filas_duplicadas']}")
    print(f"   ‚Ä¢ Calidad promedio: {100 - quality['promedio_nulos']:.1f}%")
    print()
    print(f"üìÅ Archivos generados en:")
    print(f"   ‚Ä¢ Reportes: {REPORTS_DIR}")
    print(f"   ‚Ä¢ Tablas: {TABLES_DIR}")
    print(f"   ‚Ä¢ Figuras: {FIGURES_DIR}")
    print()
    print(f"üìÑ Resumen completo disponible en:")
    print(f"   {SUMMARY_FILE}")
    print()
    print("="*70)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()

# ============================================================
# FIN DEL SCRIPT
# ============================================================